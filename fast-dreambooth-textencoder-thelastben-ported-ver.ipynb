{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.kaggle.com/code/robertodefano/fast-dreambooth-textencoder-thelastben-ported-ver?scriptVersionId=112486327\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find site-packages (note: can't be named site-packages)\n",
    "from subprocess import getoutput\n",
    "site_packages = getoutput('pip show torch')\n",
    "site_packages = site_packages[site_packages.find('Location:')+10:]\n",
    "site_packages = site_packages[:site_packages.find('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find site-packages (note: can't be named site-packages)\n",
    "from subprocess import getoutput\n",
    "site_packages = getoutput('pip show torch')\n",
    "site_packages = site_packages[site_packages.find('Location:')+10:]\n",
    "site_packages = site_packages[:site_packages.find('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/content/gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/content/gdrive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working/content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/TheLastBen/diffusers\n",
    "!pip install -q git+https://github.com/TheLastBen/diffusers\n",
    "!pip install -q accelerate==0.12.0\n",
    "!pip install -q OmegaConf\n",
    "!pip install -q wget\n",
    "!pip install -q torchsde\n",
    "!pip install -q pytorch_lightning\n",
    "!pip install -q huggingface_hub\n",
    "!pip install -U -q --no-cache-dir gdown\n",
    "!wget https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv Deps Deps.7z\n",
    "!7z x Deps.7z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/working/content/usr/local/lib/python3.7/dist-packages/* $site_packages --no-clobber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /kaggle/working/content/Deps.7z\n",
    "!rm -r /kaggle/working/content/usr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown # xformers\n",
    "\n",
    "from subprocess import getoutput\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "import wget\n",
    "import time\n",
    "\n",
    "s = getoutput('nvidia-smi')\n",
    "if 'T4' in s:\n",
    "  gpu = 'T4'\n",
    "elif 'P100' in s:\n",
    "  gpu = 'P100'\n",
    "elif 'V100' in s:\n",
    "  gpu = 'V100'\n",
    "elif 'A100' in s:\n",
    "  gpu = 'A100'\n",
    "\n",
    "while True:\n",
    "    try: \n",
    "        gpu=='T4'or gpu=='P100'or gpu=='V100'or gpu=='A100'\n",
    "        break\n",
    "    except:\n",
    "        pass\n",
    "    print('\u001b[1;31mit seems that your GPU is not supported at the moment')\n",
    "    time.sleep(5)\n",
    "\n",
    "if (gpu=='T4'):\n",
    "  !pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "  \n",
    "elif (gpu=='P100'):\n",
    "  !pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "\n",
    "elif (gpu=='V100'):\n",
    "  !pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "\n",
    "elif (gpu=='A100'):\n",
    "  !pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "\n",
    "clear_output()\n",
    "print('\u001b[1;32mDONE !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working/content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install git-lfs\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from IPython.utils import capture\n",
    "\n",
    "#@markdown - Skip this cell if you are loading a previous session\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "with capture.capture_output() as cap: \n",
    "  %cd /kaggle/working/content/\n",
    "\n",
    "Huggingface_Token = \"your_huggingface_write_token\" #@param {type:\"string\"}\n",
    "token=Huggingface_Token\n",
    "\n",
    "#@markdown (Make sure you've accepted the terms in https://huggingface.co/runwayml/stable-diffusion-v1-5)\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "#@markdown Or\n",
    "\n",
    "Path_to_HuggingFace= \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown - Load and finetune a model from Hugging Face, use the format \"profile/model\" like : runwayml/stable-diffusion-v1-5\n",
    "\n",
    "#@markdown Or\n",
    "\n",
    "CKPT_Path = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown Or\n",
    "\n",
    "CKPT_Link = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown - A CKPT direct link, huggingface CKPT link or a shared CKPT from gdrive.\n",
    "#@markdown ---\n",
    "\n",
    "Compatibility_Mode=\"\" #@param {type:\"boolean\"}\n",
    "#@markdown - Enable only if you're getting conversion errors.\n",
    "\n",
    "\n",
    "def downloadmodel():\n",
    "  token=Huggingface_Token\n",
    "  if token==\"\":\n",
    "      token=input(\"Insert your huggingface token :\")\n",
    "  if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5'):\n",
    "    !rm -r /kaggle/working/content/stable-diffusion-v1-5\n",
    "  clear_output()\n",
    "\n",
    "  %cd /kaggle/working/content/\n",
    "  clear_output()\n",
    "  !mkdir /kaggle/working/content/stable-diffusion-v1-5\n",
    "  %cd /kaggle/working/content/stable-diffusion-v1-5\n",
    "  !git init\n",
    "  !git lfs install --system --skip-repo\n",
    "  !git remote add -f origin  \"https://USER:{token}@huggingface.co/runwayml/stable-diffusion-v1-5\"\n",
    "  !git config core.sparsecheckout true\n",
    "  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nmodel_index.json\" > .git/info/sparse-checkout\n",
    "  !git pull origin main\n",
    "  if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "    !git clone \"https://USER:{token}@huggingface.co/stabilityai/sd-vae-ft-mse\"\n",
    "    !mv /kaggle/working/content/stable-diffusion-v1-5/sd-vae-ft-mse /kaggle/working/content/stable-diffusion-v1-5/vae\n",
    "    !rm -r /kaggle/working/content/stable-diffusion-v1-5/.git\n",
    "    %cd /kaggle/working/content/stable-diffusion-v1-5    \n",
    "    !rm model_index.json\n",
    "    time.sleep(1)    \n",
    "    wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n",
    "    !sed -i 's@\"clip_sample\": false@@g' /kaggle/working/content/stable-diffusion-v1-5/scheduler/scheduler_config.json\n",
    "    !sed -i 's@\"trained_betas\": null,@\"trained_betas\": null@g' /kaggle/working/content/stable-diffusion-v1-5/scheduler/scheduler_config.json\n",
    "    !sed -i 's@\"sample_size\": 256,@\"sample_size\": 512,@g' /kaggle/working/content/stable-diffusion-v1-5/vae/config.json  \n",
    "    %cd /kaggle/working/content/    \n",
    "    clear_output()\n",
    "    print('\u001b[1;32mDONE !')\n",
    "  else:\n",
    "    while not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "         print('\u001b[1;31mMake sure you accepted the terms in https://huggingface.co/runwayml/stable-diffusion-v1-5')\n",
    "         time.sleep(5)\n",
    "\n",
    "def downloadmodel_hf():\n",
    "  if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5'):\n",
    "    !rm -r /kaggle/working/content/stable-diffusion-v1-5\n",
    "  clear_output()\n",
    "\n",
    "  %cd /kaggle/working/content/\n",
    "  clear_output()\n",
    "  !mkdir /kaggle/working/content/stable-diffusion-v1-5\n",
    "  %cd /kaggle/working/content/stable-diffusion-v1-5\n",
    "  !git init\n",
    "  !git lfs install --system --skip-repo\n",
    "  !git remote add -f origin  \"https://USER:{token}@huggingface.co/{Path_to_HuggingFace}\"\n",
    "  !git config core.sparsecheckout true\n",
    "  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nmodel_index.json\" > .git/info/sparse-checkout\n",
    "  !git pull origin main\n",
    "  if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "    !git clone \"https://USER:{token}@huggingface.co/stabilityai/sd-vae-ft-mse\"\n",
    "    !mv /kaggle/working/content/stable-diffusion-v1-5/sd-vae-ft-mse /kaggle/working/content/stable-diffusion-v1-5/vae\n",
    "    !rm -r /kaggle/working/content/stable-diffusion-v1-5/.git\n",
    "    %cd /kaggle/working/content/stable-diffusion-v1-5    \n",
    "    !rm model_index.json\n",
    "    time.sleep(1)\n",
    "    wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n",
    "    !sed -i 's@\"clip_sample\": false@@g' /kaggle/working/content/stable-diffusion-v1-5/scheduler/scheduler_config.json\n",
    "    !sed -i 's@\"trained_betas\": null,@\"trained_betas\": null@g' /kaggle/working/content/stable-diffusion-v1-5/scheduler/scheduler_config.json\n",
    "    !sed -i 's@\"sample_size\": 256,@\"sample_size\": 512,@g' /kaggle/working/content/stable-diffusion-v1-5/vae/config.json    \n",
    "    %cd /kaggle/working/content/    \n",
    "    clear_output()\n",
    "    print('\u001b[1;32mDONE !')\n",
    "  else:\n",
    "    while not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "         print('\u001b[1;31mCheck the link you provided')\n",
    "         time.sleep(5)\n",
    "\n",
    "if Path_to_HuggingFace != \"\":\n",
    "  downloadmodel_hf()\n",
    "\n",
    "elif CKPT_Path !=\"\":\n",
    "  if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5'):\n",
    "    !rm -r /kaggle/working/content/stable-diffusion-v1-5\n",
    "  if os.path.exists(str(CKPT_Path)):\n",
    "    !mkdir /kaggle/working/content/stable-diffusion-v1-5\n",
    "    with capture.capture_output() as cap:\n",
    "      if Compatibility_Mode:\n",
    "        !wget https://raw.githubusercontent.com/huggingface/diffusers/039958eae55ff0700cfb42a7e72739575ab341f1/scripts/convert_original_stable_diffusion_to_diffusers.py\n",
    "        !python /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$CKPT_Path\" --dump_path /kaggle/working/content/stable-diffusion-v1-5\n",
    "        !rm /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py\n",
    "      else:           \n",
    "        !python /kaggle/working/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$CKPT_Path\" --dump_path /kaggle/working/content/stable-diffusion-v1-5        \n",
    "    if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "      !rm /kaggle/working/content/v1-inference.yaml\n",
    "      clear_output()\n",
    "      print('\u001b[1;32mDONE !')\n",
    "    else:\n",
    "      !rm /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py\n",
    "      !rm /kaggle/working/content/v1-inference.yaml\n",
    "      !rm -r /kaggle/working/content/stable-diffusion-v1-5\n",
    "      while not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "        print('\u001b[1;31mConversion error, Insufficient RAM or corrupt CKPT, use a 4.7GB CKPT instead of 7GB')\n",
    "        time.sleep(5)\n",
    "  else:\n",
    "    while not os.path.exists(str(CKPT_Path)):\n",
    "       print('\u001b[1;31mWrong path, use the colab file explorer to copy the path')\n",
    "       time.sleep(5)\n",
    "\n",
    "\n",
    "elif CKPT_Link !=\"\":   \n",
    "    if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5'):\n",
    "      !rm -r /kaggle/working/content/stable-diffusion-v1-5     \n",
    "    !gdown --fuzzy $CKPT_Link -O model.ckpt    \n",
    "    if os.path.exists('/kaggle/working/content/model.ckpt'):\n",
    "      if os.path.getsize(\"/kaggle/working/content/model.ckpt\") > 1810671599:\n",
    "        !mkdir /kaggle/working/content/stable-diffusion-v1-5\n",
    "        with capture.capture_output() as cap: \n",
    "          if Compatibility_Mode:\n",
    "            !wget https://raw.githubusercontent.com/huggingface/diffusers/039958eae55ff0700cfb42a7e72739575ab341f1/scripts/convert_original_stable_diffusion_to_diffusers.py\n",
    "            !python /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path /kaggle/working/content/model.ckpt --dump_path /kaggle/working/content/stable-diffusion-v1-5\n",
    "            !rm /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py            \n",
    "          else:           \n",
    "            !python /kaggle/working/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path /kaggle/working/content/model.ckpt --dump_path /kaggle/working/content/stable-diffusion-v1-5\n",
    "        if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "          clear_output()\n",
    "          print('\u001b[1;32mDONE !')\n",
    "          !rm /kaggle/working/content/v1-inference.yaml\n",
    "          !rm /kaggle/working/content/model.ckpt\n",
    "        else:\n",
    "          if os.path.exists('/kaggle/working/content/v1-inference.yaml'):\n",
    "            !rm /kaggle/working/content/v1-inference.yaml\n",
    "          !rm /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py\n",
    "          !rm -r /kaggle/working/content/stable-diffusion-v1-5\n",
    "          !rm /kaggle/working/content/model.ckpt\n",
    "          while not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "            print('\u001b[1;31mConversion error, Insufficient RAM or corrupt CKPT, use a 4.7GB CKPT instead of 7GB')\n",
    "            time.sleep(5)\n",
    "      else:\n",
    "        while os.path.getsize('/kaggle/working/content/model.ckpt') < 1810671599:\n",
    "           print('\u001b[1;31mWrong link, check that the link is valid')\n",
    "           time.sleep(5)\n",
    "else:\n",
    "  downloadmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/content/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/instance_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/Regularization_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "from IPython.utils import capture\n",
    "import wget\n",
    "import time\n",
    "\n",
    "#@markdown #Create/Load a Session\n",
    "\n",
    "def fdownloadmodel():\n",
    "    token=input(\"Insert your huggingface token :\")\n",
    "    %cd /kaggle/working/content/\n",
    "    clear_output()\n",
    "    !mkdir /kaggle/working/content/stable-diffusion-v1-5\n",
    "    %cd /kaggle/working/content/stable-diffusion-v1-5\n",
    "    !git init\n",
    "    !git lfs install --system --skip-repo\n",
    "    !git remote add -f origin  \"https://USER:{token}@huggingface.co/runwayml/stable-diffusion-v1-5\"\n",
    "    !git config core.sparsecheckout true\n",
    "    !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nmodel_index.json\" > .git/info/sparse-checkout\n",
    "    !git pull origin main\n",
    "    if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "      !git clone \"https://USER:{token}@huggingface.co/stabilityai/sd-vae-ft-mse\"\n",
    "      !mv /kaggle/working/content/stable-diffusion-v1-5/sd-vae-ft-mse /kaggle/working/content/stable-diffusion-v1-5/vae\n",
    "      !rm -r /kaggle/working/content/stable-diffusion-v1-5/.git\n",
    "      %cd /kaggle/working/content/    \n",
    "      !rm model_index.json\n",
    "      time.sleep(1)    \n",
    "      wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n",
    "      !sed -i 's@\"clip_sample\": false@@g' /kaggle/working/content/stable-diffusion-v1-5/scheduler/scheduler_config.json\n",
    "      !sed -i 's@\"trained_betas\": null,@\"trained_betas\": null@g' /kaggle/working/content/stable-diffusion-v1-5/scheduler/scheduler_config.json\n",
    "      !sed -i 's@\"sample_size\": 256,@\"sample_size\": 512,@g' /kaggle/working/content/stable-diffusion-v1-5/vae/config.json  \n",
    "      %cd /kaggle/working/content/    \n",
    "      clear_output()\n",
    "      print('\u001b[1;32mDONE !')\n",
    "    else:\n",
    "      while not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "           print('\u001b[1;31mMake sure you accepted the terms in https://huggingface.co/runwayml/stable-diffusion-v1-5')\n",
    "           time.sleep(5)\n",
    "\n",
    "MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v1-5\"\n",
    "PT=\"\"\n",
    "\n",
    "Captionned_instance_images = True\n",
    "\n",
    "Session_Name = \"your_instance_name\" #@param{type: 'string'}\n",
    "while Session_Name==\"\":\n",
    "  print('\u001b[1;31mInput the Session Name:') \n",
    "  Session_Name=input('')\n",
    "Session_Name=Session_Name.replace(\" \",\"_\")\n",
    "\n",
    "#@markdown - Enter the session name, it if it exists, it will load it, otherwise it'll create an new session.\n",
    "\n",
    "Session_Link_optional = \"\" #@param{type: 'string'}\n",
    "\n",
    "#@markdown - Import a session from another gdrive, the shared gdrive link must point to the specific session's folder that contains the trained CKPT, remove any intermediary CKPT if any.\n",
    "\n",
    "WORKSPACE='/kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth'\n",
    "\n",
    "if Session_Link_optional !=\"\":\n",
    "  print('\u001b[1;32mDownloading session...')\n",
    "with capture.capture_output() as cap:\n",
    "  %cd /kaggle/working/content\n",
    "  if Session_Link_optional != \"\":\n",
    "    if not os.path.exists(str(WORKSPACE+'/Sessions')):\n",
    "      %mkdir -p $WORKSPACE'/Sessions'\n",
    "      time.sleep(1)\n",
    "    %cd $WORKSPACE'/Sessions'\n",
    "    !gdown --folder --remaining-ok -O $Session_Name  $Session_Link_optional\n",
    "    %cd $Session_Name\n",
    "    !rm -r instance_images\n",
    "    !rm -r Regularization_images\n",
    "    !unzip instance_images.zip\n",
    "    !mv *.ckpt $Session_Name\".ckpt\"\n",
    "    %cd /kaggle/working/content\n",
    "\n",
    "    \n",
    "INSTANCE_NAME=Session_Name    \n",
    "OUTPUT_DIR=\"/kaggle/working/content/models/\"+Session_Name\n",
    "SESSION_DIR=WORKSPACE+'/Sessions/'+Session_Name\n",
    "INSTANCE_DIR=SESSION_DIR+'/instance_images'\n",
    "MDLPTH=str(SESSION_DIR+\"/\"+Session_Name+'.ckpt')\n",
    "CLASS_DIR=SESSION_DIR+'/Regularization_images'\n",
    "\n",
    "Contains_faces = \"No\"\n",
    "\n",
    "def reg():\n",
    "  with capture.capture_output() as cap:\n",
    "    if Contains_faces!=\"No\":\n",
    "      if not os.path.exists(str(CLASS_DIR)):\n",
    "        %mkdir -p \"$CLASS_DIR\"\n",
    "      %cd $CLASS_DIR\n",
    "      !rm -r Women Men Mix\n",
    "      !wget -O Womenz 'https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/Regularization/Women'\n",
    "      !wget -O Menz 'https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/Regularization/Men'\n",
    "      !wget -O Mixz 'https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/Regularization/Mix'\n",
    "      !unzip Menz\n",
    "      !unzip Womenz\n",
    "      !unzip Mixz\n",
    "      !rm Menz Womenz Mixz\n",
    "      !find . -name \"* *\" -type f | rename 's/ /_/g'\n",
    "      %cd /kaggle/working/content\n",
    "            \n",
    "if os.path.exists(str(SESSION_DIR)) and not os.path.exists(str(SESSION_DIR+\"/\"+Session_Name+'.ckpt')):\n",
    "  print('\u001b[1;32mLoading session with no previous model, using the original model')\n",
    "  reg()\n",
    "  if not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "    if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5'):\n",
    "      !rm -r '/kaggle/working/content/stable-diffusion-v1-5'    \n",
    "    fdownloadmodel()\n",
    "  if not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "    print('\u001b[1;31mError downloading the model, make sure you have accepted the terms at https://huggingface.co/runwayml/stable-diffusion-v1-5')\n",
    "  else:\n",
    "    print('\u001b[1;32mSession Loaded, proceed to uploading instance images')\n",
    "\n",
    "elif os.path.exists(str(SESSION_DIR+\"/\"+Session_Name+'.ckpt')):\n",
    "  print('\u001b[1;32mSession found, loading the trained model ...')\n",
    "  reg()\n",
    "  %mkdir -p \"$OUTPUT_DIR\"\n",
    "  !python /kaggle/working/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$MDLPTH\" --dump_path \"$OUTPUT_DIR\" --session_dir \"$SESSION_DIR\"\n",
    "  if os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n",
    "    resume=True    \n",
    "    !rm /kaggle/working/content/v1-inference.yaml\n",
    "    clear_output()\n",
    "    print('\u001b[1;32mSession loaded.')\n",
    "  else:     \n",
    "    !rm /kaggle/working/content/v1-inference.yaml\n",
    "    if not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n",
    "      print('\u001b[1;31mConversion error, if the error persists, remove the CKPT file from the current session folder')\n",
    "\n",
    "\n",
    "elif not os.path.exists(str(SESSION_DIR)):\n",
    "    %mkdir -p \"$INSTANCE_DIR\"\n",
    "    print('\u001b[1;32mCreating session...')\n",
    "    reg()\n",
    "    if not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "      if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5'):\n",
    "        !rm -r '/kaggle/working/content/stable-diffusion-v1-5'\n",
    "      fdownloadmodel()\n",
    "    if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "      print('\u001b[1;32mSession created, proceed to uploading instance images')\n",
    "    else:\n",
    "      print('\u001b[1;31mError downloading the model, make sure you have accepted the terms at https://huggingface.co/runwayml/stable-diffusion-v1-5')  \n",
    "    \n",
    "if Contains_faces == \"Female\":\n",
    "  CLASS_DIR=CLASS_DIR+'/Women'\n",
    "if Contains_faces == \"Male\":\n",
    "  CLASS_DIR=CLASS_DIR+'/Men'\n",
    "if Contains_faces == \"Both\":\n",
    "  CLASS_DIR=CLASS_DIR+'/Mix'\n",
    "\n",
    "try:\n",
    "  Contain_f\n",
    "  del Contain_f\n",
    "except:\n",
    "  pass\n",
    "\n",
    "    #@markdown \n",
    "\n",
    "    #@markdown # The most importent step is to rename the instance picture to the same instance unique identifier for each subject, example :\n",
    "    #@markdown - If you have 30 pictures of yourself, simply select them all and rename only one to the chosen identifier for example : phtmejhn, the files would be : phtmejhn (1).jpg, phtmejhn (2).png ....etc then upload them, do the same for other people or objects with a different identifier, and that's it.\n",
    "    #@markdown - Check out this example : https://i.imgur.com/d2lD3rz.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/your_images_data_folder /kaggle/working/my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown #Instance Images\n",
    "#@markdown ----\n",
    "\n",
    "#@markdown\n",
    "#@markdown - Run the cell to Upload the instance pictures.\n",
    "\n",
    "Remove_existing_instance_images= True #@param{type: 'boolean'}\n",
    "#@markdown - Uncheck the box to keep the existing instance images.\n",
    "\n",
    "\n",
    "if Remove_existing_instance_images:\n",
    "  if os.path.exists(str(INSTANCE_DIR)):\n",
    "    !rm -r \"$INSTANCE_DIR\"\n",
    "\n",
    "if not os.path.exists(str(INSTANCE_DIR)):\n",
    "  %mkdir -p \"$INSTANCE_DIR\"\n",
    "\n",
    "IMAGES_FOLDER_OPTIONAL=\"/kaggle/working/my\" #@param{type: 'string'}\n",
    "\n",
    "#@markdown - If you prefer to specify directly the folder of the pictures instead of uploading, this will add the pictures to the existing (if any) instance images. Leave EMPTY to upload.\n",
    "\n",
    "Crop_images= True #@param{type: 'boolean'}\n",
    "Crop_size = \"512\" #@param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"]\n",
    "Crop_size=int(Crop_size)\n",
    "\n",
    "#@markdown - Unless you want to crop them manually in a precise way, you don't need to crop your instance images externally.\n",
    "\n",
    "while IMAGES_FOLDER_OPTIONAL !=\"\" and not os.path.exists(str(IMAGES_FOLDER_OPTIONAL)):\n",
    "  print('\u001b[1;31mThe image folder specified does not exist, use the colab file explorer to copy the path :')\n",
    "  IMAGES_FOLDER_OPTIONAL=input('')\n",
    "\n",
    "if IMAGES_FOLDER_OPTIONAL!=\"\":\n",
    "  if Crop_images:\n",
    "    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
    "      extension = filename.split(\".\")[1]\n",
    "      identifier=filename.split(\".\")[0]\n",
    "      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n",
    "      file = Image.open(IMAGES_FOLDER_OPTIONAL+\"/\"+filename)\n",
    "      width, height = file.size\n",
    "      if file.size !=(Crop_size, Crop_size):      \n",
    "        side_length = min(width, height)\n",
    "        left = (width - side_length)/2\n",
    "        top = (height - side_length)/2\n",
    "        right = (width + side_length)/2\n",
    "        bottom = (height + side_length)/2\n",
    "        image = file.crop((left, top, right, bottom))\n",
    "        image = image.resize((Crop_size, Crop_size))\n",
    "        if (extension.upper() == \"JPG\"):\n",
    "            image.save(new_path_with_file, format=\"JPEG\", quality = 100)\n",
    "        else:\n",
    "            image.save(new_path_with_file, format=extension.upper())\n",
    "      else:\n",
    "        !cp \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n",
    "\n",
    "  else:\n",
    "    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
    "      %cp -r \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n",
    " \n",
    "  print('\\n\u001b[1;32mDone, proceed to the training cell')\n",
    "\n",
    "\n",
    "elif IMAGES_FOLDER_OPTIONAL ==\"\":\n",
    "  uploaded = files.upload()\n",
    "  if Crop_images:\n",
    "    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
    "      shutil.move(filename, INSTANCE_DIR)\n",
    "      extension = filename.split(\".\")[1]\n",
    "      identifier=filename.split(\".\")[0]\n",
    "      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n",
    "      file = Image.open(new_path_with_file)\n",
    "      width, height = file.size\n",
    "      if file.size !=(Crop_size, Crop_size):        \n",
    "        side_length = min(width, height)\n",
    "        left = (width - side_length)/2\n",
    "        top = (height - side_length)/2\n",
    "        right = (width + side_length)/2\n",
    "        bottom = (height + side_length)/2\n",
    "        image = file.crop((left, top, right, bottom))\n",
    "        image = image.resize((Crop_size, Crop_size))\n",
    "        if (extension.upper() == \"JPG\"):\n",
    "            image.save(new_path_with_file, format=\"JPEG\", quality = 100)\n",
    "        else:\n",
    "            image.save(new_path_with_file, format=extension.upper())\n",
    "      else:\n",
    "          image.save(new_path_with_file, format=extension.upper())\n",
    "      clear_output()\n",
    "  else:\n",
    "    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
    "      shutil.move(filename, INSTANCE_DIR)\n",
    "      clear_output()\n",
    "\n",
    "  print('\\n\u001b[1;32mDone, proceed to the training cell')\n",
    "\n",
    "with capture.capture_output() as cap:\n",
    "  %cd \"$INSTANCE_DIR\"\n",
    "  !find . -name \"* *\" -type f | rename 's/ /-/g'\n",
    "  %cd /content\n",
    "  if os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n",
    "    %rm -r INSTANCE_DIR+\"/.ipynb_checkpoints\"    \n",
    "\n",
    "  %cd $SESSION_DIR\n",
    "  !rm instance_images.zip\n",
    "  !zip -r instance_images instance_images\n",
    "  %cd /kaggle/working/content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers\"[training]\" accelerate \"transformers>=4.21.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers\"[training]\" accelerate \"transformers>=4.21.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working/content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ---\n",
    "#@markdown #Start DreamBooth\n",
    "#@markdown ---\n",
    "import os\n",
    "from subprocess import getoutput\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "Resume_Training = False #@param {type:\"boolean\"}\n",
    "\n",
    "if not Resume_Training and not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "  if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5'):\n",
    "    !rm -r '/kaggle/working/content/stable-diffusion-v1-5'\n",
    "  print('\u001b[1;31mOriginal model not found, downloading....\u001b[0m')\n",
    "  fdownloadmodel()\n",
    "  if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "     print('\u001b[1;32mModel downloaded, proceeding to training...')\n",
    "  else:\n",
    "     print('\u001b[1;31mError downloading the model, make sure you have accepted the terms at https://huggingface.co/runwayml/stable-diffusion-v1-5')  \n",
    "\n",
    "#@markdown  - If you're not satisfied with the result, check this box, run again the cell and it will continue training the current model.\n",
    "\n",
    "MODELT_NAME=MODEL_NAME\n",
    "\n",
    "Training_Steps=3000 #@param{type: 'number'}\n",
    "#@markdown - Total Steps = Number of Instance images * 200, if you use 30 images, use 6000 steps, if you're not satisfied with the result, resume training for another 500 steps, and so on ...\n",
    "\n",
    "Seed='' #@param{type: 'number'}\n",
    "\n",
    "#@markdown - Leave empty for a random seed.\n",
    "\n",
    "Resolution = \"512\" #@param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"]\n",
    "Res=int(Resolution)\n",
    "\n",
    "#@markdown - Higher resolution = Higher quality, make sure the instance images are cropped to this selected size (or larger), if you're getting memory issues, check the box below (slower speed but memory effecient) :\n",
    "\n",
    "Reduce_memory_usage = False #@param {type:\"boolean\"}\n",
    "\n",
    "fp16 = True #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown - Enable/disable half-precision, disabling it will double the training time and produce 4.7Gb checkpoints.\n",
    "\n",
    "GC= \"\"\n",
    "if Reduce_memory_usage:\n",
    "  GC= \"--gradient_checkpointing\"\n",
    "\n",
    "if Seed =='' or Seed=='0':\n",
    "  Seed=random.randint(1, 999999)\n",
    "else:\n",
    "  Seed=int(Seed)\n",
    "\n",
    "if fp16:\n",
    "  prec=\"fp16\"\n",
    "else:\n",
    "  prec=\"no\"\n",
    "\n",
    "s = getoutput('nvidia-smi')\n",
    "if 'A100' in s:\n",
    "  precision=\"no\"\n",
    "else:\n",
    "  precision=prec\n",
    "\n",
    "try:\n",
    "   resume\n",
    "   if resume and not Resume_Training:\n",
    "     print('\u001b[1;31mOverwrite your previously trained model ?, answering \"yes\" will train a new model, answering \"no\" will resume the training of the previous model?  yes or no ?\u001b[0m')\n",
    "     while True:\n",
    "        ansres=input('')\n",
    "        if ansres=='no':\n",
    "          Resume_Training = True\n",
    "          del ansres\n",
    "          break\n",
    "        elif ansres=='yes':\n",
    "          Resume_Training = False\n",
    "          resume= False\n",
    "          break\n",
    "except:\n",
    "  pass\n",
    "\n",
    "if Resume_Training and os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n",
    "  MODELT_NAME=OUTPUT_DIR\n",
    "  print('\u001b[1;32mResuming Training...\u001b[0m')\n",
    "elif Resume_Training and not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n",
    "  print('\u001b[1;31mPrevious model not found, training a new model...\u001b[0m') \n",
    "  MODELT_NAME=MODEL_NAME\n",
    "\n",
    "#@markdown ---------------------------\n",
    "\n",
    "try:\n",
    "   Contain_f\n",
    "   pass\n",
    "except:\n",
    "   Contain_f=Contains_faces\n",
    "\n",
    "Enable_text_encoder_training= True #@param{type: 'boolean'}\n",
    "\n",
    "#@markdown - At least 10% of the total training steps are needed, it doesn't matter if they are at the beginning or in the middle or the end, in case you're training the model multiple times.\n",
    "#@markdown - For example you can devide 5%, 5%, 5% on 3 training runs on the model, or 0%, 0%, 15%, given that 15% will cover the total training steps count (15% of 200 steps is not enough).\n",
    "\n",
    "#@markdown - Enter the % of the total steps for which to train the text_encoder\n",
    "Train_text_encoder_for=30 #@param{type: 'number'}\n",
    "\n",
    "#@markdown - If you're training a style, keep it between 10-20%, if you're training on a person, set it between 50-70%, reduce it if you can't stylize the person/object.\n",
    "#@markdown - Higher % will give more weight to the instance, it gives stronger results at lower steps count, but harder to stylize.\n",
    "\n",
    "if Train_text_encoder_for>=100:\n",
    "  stptxt=Training_Steps\n",
    "elif Train_text_encoder_for==0:\n",
    "  Enable_text_encoder_training= False\n",
    "  stptxt=10\n",
    "else:\n",
    "  stptxt=int((Training_Steps*Train_text_encoder_for)/100)\n",
    "\n",
    "if not Enable_text_encoder_training:\n",
    "  Contains_faces=\"No\"\n",
    "else:\n",
    "   Contains_faces=Contain_f\n",
    "\n",
    "if Enable_text_encoder_training:\n",
    "  Textenc=\"--train_text_encoder\"\n",
    "else:\n",
    "  Textenc=\"\"\n",
    "\n",
    "#@markdown ---------------------------\n",
    "Save_Checkpoint_Every_n_Steps = False #@param {type:\"boolean\"}\n",
    "Save_Checkpoint_Every=500 #@param{type: 'number'}\n",
    "if Save_Checkpoint_Every==None:\n",
    "  Save_Checkpoint_Every=1\n",
    "#@markdown - Minimum 200 steps between each save.\n",
    "stp=0\n",
    "Start_saving_from_the_step=500 #@param{type: 'number'}\n",
    "if Start_saving_from_the_step==None:\n",
    "  Start_saving_from_the_step=0\n",
    "if (Start_saving_from_the_step < 200):\n",
    "  Start_saving_from_the_step=Save_Checkpoint_Every\n",
    "stpsv=Start_saving_from_the_step\n",
    "if Save_Checkpoint_Every_n_Steps:\n",
    "  stp=Save_Checkpoint_Every\n",
    "#@markdown - Start saving intermediary checkpoints from this step.\n",
    "\n",
    "Caption=''\n",
    "if Captionned_instance_images:\n",
    "  Caption='--image_captions_filename'\n",
    "\n",
    "\n",
    "def txtenc_train(Caption, stpsv, stp, MODELT_NAME, INSTANCE_DIR, CLASS_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps):\n",
    "  print('\u001b[1;33mTraining the text encoder with regularization...\u001b[0m')\n",
    "  !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n",
    "    $Caption \\\n",
    "    --train_text_encoder \\\n",
    "    --dump_only_text_encoder \\\n",
    "    --pretrained_model_name_or_path=\"$MODEL_NAME\" \\\n",
    "    --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "    --class_data_dir=\"$CLASS_DIR\" \\\n",
    "    --output_dir=\"$OUTPUT_DIR\" \\\n",
    "    --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "    --instance_prompt=\"$PT\"\\\n",
    "    --seed=$Seed \\\n",
    "    --resolution=$Res \\\n",
    "    --mixed_precision=$precision \\\n",
    "    --train_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=1 --gradient_checkpointing \\\n",
    "    --use_8bit_adam \\\n",
    "    --learning_rate=2e-6 \\\n",
    "    --lr_scheduler=\"polynomial\" \\\n",
    "    --lr_warmup_steps=0 \\\n",
    "    --max_train_steps=$Training_Steps \\\n",
    "    --num_class_images=200\n",
    "\n",
    "def unet_train(Caption, SESSION_DIR, stpsv, stp, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps):\n",
    "  clear_output()\n",
    "  print('\u001b[1;33mTraining the unet...\u001b[0m')\n",
    "  !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n",
    "    $Caption \\\n",
    "    --train_only_unet \\\n",
    "    --Session_dir=$SESSION_DIR \\\n",
    "    --save_starting_step=$stpsv \\\n",
    "    --save_n_steps=$stp \\\n",
    "    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n",
    "    --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "    --output_dir=\"$OUTPUT_DIR\" \\\n",
    "    --instance_prompt=\"$PT\" \\\n",
    "    --seed=$Seed \\\n",
    "    --resolution=$Res \\\n",
    "    --mixed_precision=$precision \\\n",
    "    --train_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=1 $GC \\\n",
    "    --use_8bit_adam \\\n",
    "    --learning_rate=2e-6 \\\n",
    "    --lr_scheduler=\"polynomial\" \\\n",
    "    --lr_warmup_steps=0 \\\n",
    "    --max_train_steps=$Training_Steps\n",
    "\n",
    "if Contains_faces!=\"No\":\n",
    "  \n",
    "  txtenc_train(Caption, stpsv, stp, MODELT_NAME, INSTANCE_DIR, CLASS_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps=stptxt)\n",
    "  unet_train(Caption, SESSION_DIR, stpsv, stp, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps)\n",
    "\n",
    "else:\n",
    "  !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n",
    "    $Caption \\\n",
    "    $Textenc \\\n",
    "    --save_starting_step=$stpsv \\\n",
    "    --stop_text_encoder_training=$stptxt \\\n",
    "    --save_n_steps=$stp \\\n",
    "    --Session_dir=$SESSION_DIR \\\n",
    "    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n",
    "    --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "    --output_dir=\"$OUTPUT_DIR\" \\\n",
    "    --instance_prompt=\"$PT\" \\\n",
    "    --seed=$Seed \\\n",
    "    --resolution=$Res \\\n",
    "    --mixed_precision=$precision \\\n",
    "    --train_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=1 $GC \\\n",
    "    --use_8bit_adam \\\n",
    "    --learning_rate=2e-6 \\\n",
    "    --lr_scheduler=\"polynomial\" \\\n",
    "    --lr_warmup_steps=0 \\\n",
    "    --max_train_steps=$Training_Steps\n",
    "\n",
    "\n",
    "if os.path.exists('/kaggle/working/content/models/'+INSTANCE_NAME+'/unet/diffusion_pytorch_model.bin'):\n",
    "  print(\"Almost done ...\")\n",
    "  %cd /kaggle/working/content    \n",
    "  !wget -O convertosd.py https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/convertosd.py\n",
    "  clear_output()\n",
    "  if precision==\"no\":\n",
    "    !sed -i '226s@.*@@' /kaggle/working/content/convertosd.py\n",
    "  !sed -i '201s@.*@    model_path = \"{OUTPUT_DIR}\"@' /kaggle/working/content/convertosd.py\n",
    "  !sed -i '202s@.*@    checkpoint_path= \"{SESSION_DIR}/{Session_Name}.ckpt\"@' /kaggle/working/content/convertosd.py\n",
    "  !python /kaggle/working/content/convertosd.py\n",
    "  clear_output()\n",
    "  if os.path.exists(SESSION_DIR+\"/\"+INSTANCE_NAME+'.ckpt'):\n",
    "    if not os.path.exists(str(SESSION_DIR+'/tokenizer')):\n",
    "      !cp -R '/kaggle/working/content/models/'$INSTANCE_NAME'/tokenizer' \"$SESSION_DIR\"\n",
    "    print(\"\u001b[1;32mDONE, the CKPT model is in your Gdrive in the sessions folder\")\n",
    "  else:\n",
    "    print(\"\u001b[1;31mSomething went wrong\")\n",
    "    \n",
    "else:\n",
    "  print(\"\u001b[1;31mSomething went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/your_instance_name/your_instance_name.ckpt /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'your_instance_name.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('python-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4aa31368adc1e672aa8d54d8517f837ba19bb89b8f889a2edc152335a4833cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
