{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/robertodefano/fast-dreambooth-sdv2-model?scriptVersionId=131137986\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-05-26T13:44:08.994921Z","iopub.execute_input":"2023-05-26T13:44:08.995532Z","iopub.status.idle":"2023-05-26T13:44:09.008385Z","shell.execute_reply.started":"2023-05-26T13:44:08.995495Z","shell.execute_reply":"2023-05-26T13:44:09.007006Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip install tqdm","metadata":{"execution":{"iopub.status.busy":"2023-05-26T13:44:13.195856Z","iopub.execute_input":"2023-05-26T13:44:13.196337Z","iopub.status.idle":"2023-05-26T13:44:26.427308Z","shell.execute_reply.started":"2023-05-26T13:44:13.196296Z","shell.execute_reply":"2023-05-26T13:44:26.426079Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.64.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-05-26T14:10:31.112162Z","iopub.execute_input":"2023-05-26T14:10:31.112538Z","iopub.status.idle":"2023-05-26T14:12:54.010007Z","shell.execute_reply.started":"2023-05-26T14:10:31.112504Z","shell.execute_reply":"2023-05-26T14:12:54.008777Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\nCollecting torch==2.0.0+cu118\n  Using cached https://download.pytorch.org/whl/cu118/torch-2.0.0%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\nCollecting torchvision==0.15.1+cu118\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.1%2Bcu118-cp310-cp310-linux_x86_64.whl (6.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==2.0.1\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (4.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0+cu118) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0+cu118) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0+cu118) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0+cu118) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0+cu118) (3.1.2)\nCollecting triton==2.0.0 (from torch==2.0.0+cu118)\n  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.15.1+cu118) (1.23.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision==0.15.1+cu118) (2.28.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.15.1+cu118) (9.5.0)\nCollecting cmake (from triton==2.0.0->torch==2.0.0+cu118)\n  Downloading https://download.pytorch.org/whl/cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting lit (from triton==2.0.0->torch==2.0.0+cu118)\n  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.0+cu118) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.15.1+cu118) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.15.1+cu118) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.15.1+cu118) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.15.1+cu118) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.0+cu118) (1.3.0)\nBuilding wheels for collected packages: lit\n  Building wheel for lit (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=90003 sha256=3adca81718c3dd3ca934abe3e134e5ba1b08d3eab3f5301dab62d4c286fd677e\n  Stored in directory: /root/.cache/pip/wheels/bf/62/0d/01218f13f6a8051e982a7ce31d12b7bfd725dc69bd227ae104\nSuccessfully built lit\nInstalling collected packages: lit, cmake, triton, torch, torchvision, torchaudio\n  Attempting uninstall: torch\n    Found existing installation: torch 1.12.0+cu113\n    Uninstalling torch-1.12.0+cu113:\n      Successfully uninstalled torch-1.12.0+cu113\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.13.0+cu113\n    Uninstalling torchvision-0.13.0+cu113:\n      Successfully uninstalled torchvision-0.13.0+cu113\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 0.12.0+cu113\n    Uninstalling torchaudio-0.12.0+cu113:\n      Successfully uninstalled torchaudio-0.12.0+cu113\nSuccessfully installed cmake-3.25.0 lit-15.0.7 torch-2.0.0+cu118 torchaudio-2.0.1+cu118 torchvision-0.15.1+cu118 triton-2.0.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:19:33.722356Z","iopub.execute_input":"2023-05-26T14:19:33.722799Z","iopub.status.idle":"2023-05-26T14:19:33.731502Z","shell.execute_reply.started":"2023-05-26T14:19:33.722764Z","shell.execute_reply":"2023-05-26T14:19:33.730493Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/\n","output_type":"stream"}]},{"cell_type":"code","source":"mkdir /kaggle/working/content","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:19:37.114356Z","iopub.execute_input":"2023-05-26T14:19:37.114725Z","iopub.status.idle":"2023-05-26T14:19:38.094306Z","shell.execute_reply.started":"2023-05-26T14:19:37.114694Z","shell.execute_reply":"2023-05-26T14:19:38.093026Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:19:41.63469Z","iopub.execute_input":"2023-05-26T14:19:41.635066Z","iopub.status.idle":"2023-05-26T14:19:42.636163Z","shell.execute_reply.started":"2023-05-26T14:19:41.635035Z","shell.execute_reply":"2023-05-26T14:19:42.634864Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive/MyDrive","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:19:45.458842Z","iopub.execute_input":"2023-05-26T14:19:45.459934Z","iopub.status.idle":"2023-05-26T14:19:46.423383Z","shell.execute_reply.started":"2023-05-26T14:19:45.459898Z","shell.execute_reply":"2023-05-26T14:19:46.422006Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working/content","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:19:48.616762Z","iopub.execute_input":"2023-05-26T14:19:48.617146Z","iopub.status.idle":"2023-05-26T14:19:48.623622Z","shell.execute_reply.started":"2023-05-26T14:19:48.617113Z","shell.execute_reply":"2023-05-26T14:19:48.622669Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"/kaggle/working/content\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow-io-gcs-filesystem==0.31.0","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:21:17.547758Z","iopub.execute_input":"2023-05-26T14:21:17.548839Z","iopub.status.idle":"2023-05-26T14:21:29.455295Z","shell.execute_reply.started":"2023-05-26T14:21:17.548798Z","shell.execute_reply":"2023-05-26T14:21:29.454196Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Collecting tensorflow-io-gcs-filesystem==0.31.0\n  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: tensorflow-io-gcs-filesystem\n  Attempting uninstall: tensorflow-io-gcs-filesystem\n    Found existing installation: tensorflow-io-gcs-filesystem 0.32.0\n    Uninstalling tensorflow-io-gcs-filesystem-0.32.0:\n      Successfully uninstalled tensorflow-io-gcs-filesystem-0.32.0\nSuccessfully installed tensorflow-io-gcs-filesystem-0.31.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install google-cloud-bigquery-storage","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:21:34.612865Z","iopub.execute_input":"2023-05-26T14:21:34.613919Z","iopub.status.idle":"2023-05-26T14:21:55.073955Z","shell.execute_reply.started":"2023-05-26T14:21:34.613877Z","shell.execute_reply":"2023-05-26T14:21:55.072855Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Collecting google-cloud-bigquery-storage\n  Downloading google_cloud_bigquery_storage-2.19.1-py2.py3-none-any.whl (190 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.1/190.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 (from google-cloud-bigquery-storage)\n  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.3/120.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery-storage) (1.22.2)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery-storage) (3.20.3)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (1.57.1)\nRequirement already satisfied: google-auth<3.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (2.17.3)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (2.28.2)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (1.51.1)\nRequirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (1.48.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (0.2.7)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (1.16.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (4.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (2023.5.7)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (0.4.8)\nInstalling collected packages: google-api-core, google-cloud-bigquery-storage\n  Attempting uninstall: google-api-core\n    Found existing installation: google-api-core 1.33.2\n    Uninstalling google-api-core-1.33.2:\n      Successfully uninstalled google-api-core-1.33.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.0 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.0 which is incompatible.\ngoogle-cloud-pubsub 2.16.1 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\nkfp 1.8.21 requires google-api-python-client<2,>=1.7.8, but you have google-api-python-client 2.86.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed google-api-core-2.11.0 google-cloud-bigquery-storage-2.19.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wget","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:23:00.282609Z","iopub.execute_input":"2023-05-26T14:23:00.283653Z","iopub.status.idle":"2023-05-26T14:23:13.32016Z","shell.execute_reply.started":"2023-05-26T14:23:00.283603Z","shell.execute_reply":"2023-05-26T14:23:13.319076Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Collecting wget\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=92fec9aac0d595ef10f3c1e9f0efbc3c90b70d8778f86998770e65f9ff3506a2\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built wget\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"#@markdown # Dependencies\nfrom subprocess import getoutput\nimport time\n\n%cd /kaggle/working/content/\n!pip install -q accelerate==0.12.0\nfor i in range(1,7):\n    !wget \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dependencies/Dependencies_AUT.{i}\"\n    !mv \"Dependencies_AUT.{i}\" \"Dependencies_AUT.7z.00{i}\"\n!7z x Dependencies_AUT.7z.001\ntime.sleep(2)\n!cp -r /kaggle/working/content/usr/local/lib/python3.8/dist-packages /usr/local/lib/python3.8/\n!rm -r /kaggle/working/content/usr\nfor i in range(1,7):\n    !rm \"Dependencies_AUT.7z.00{i}\"\n!pip uninstall -y diffusers\n!git clone --branch updt https://github.com/TheLastBen/diffusers\n!pip install -q /kaggle/working/content/diffusers\n!pip install -q -U pillow\ns = getoutput('nvidia-smi')\nif \"A100\" in s:\n    !wget https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/A100\n    %cd /kaggle/working/usr/local/lib/python3.8/dist-packages/xformers\n    !7z x -y /kaggle/working/content/A100\n    !rm /kaggle/working/content/A100\nif not (\"T4\" in s or \"A100\" in s):\n    !pip uninstall -q -y xformers","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:23:20.994549Z","iopub.execute_input":"2023-05-26T14:23:20.99497Z","iopub.status.idle":"2023-05-26T14:24:41.57854Z","shell.execute_reply.started":"2023-05-26T14:23:20.994936Z","shell.execute_reply":"2023-05-26T14:24:41.577367Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"/kaggle/working/content\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m--2023-05-26 14:23:32--  https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dependencies/Dependencies_AUT.1\nResolving github.com (github.com)... 20.27.177.113\nConnecting to github.com (github.com)|20.27.177.113|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2023-05-26 14:23:33 ERROR 404: Not Found.\n\nmv: cannot stat 'Dependencies_AUT.1': No such file or directory\n--2023-05-26 14:23:35--  https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dependencies/Dependencies_AUT.2\nResolving github.com (github.com)... 20.27.177.113\nConnecting to github.com (github.com)|20.27.177.113|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2023-05-26 14:23:35 ERROR 404: Not Found.\n\nmv: cannot stat 'Dependencies_AUT.2': No such file or directory\n--2023-05-26 14:23:38--  https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dependencies/Dependencies_AUT.3\nResolving github.com (github.com)... 20.27.177.113\nConnecting to github.com (github.com)|20.27.177.113|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2023-05-26 14:23:38 ERROR 404: Not Found.\n\nmv: cannot stat 'Dependencies_AUT.3': No such file or directory\n--2023-05-26 14:23:40--  https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dependencies/Dependencies_AUT.4\nResolving github.com (github.com)... 20.27.177.113\nConnecting to github.com (github.com)|20.27.177.113|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2023-05-26 14:23:41 ERROR 404: Not Found.\n\nmv: cannot stat 'Dependencies_AUT.4': No such file or directory\n--2023-05-26 14:23:43--  https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dependencies/Dependencies_AUT.5\nResolving github.com (github.com)... 20.27.177.113\nConnecting to github.com (github.com)|20.27.177.113|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2023-05-26 14:23:43 ERROR 404: Not Found.\n\nmv: cannot stat 'Dependencies_AUT.5': No such file or directory\n--2023-05-26 14:23:45--  https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dependencies/Dependencies_AUT.6\nResolving github.com (github.com)... 20.27.177.113\nConnecting to github.com (github.com)|20.27.177.113|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2023-05-26 14:23:46 ERROR 404: Not Found.\n\nmv: cannot stat 'Dependencies_AUT.6': No such file or directory\n\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=C.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.00GHz (50653),ASM,AES-NI)\n\nScanning the drive for archives:\n  0M Sca        \nERROR: No more files\nDependencies_AUT.7z.001\n\n\n\nSystem ERROR:\nUnknown error -2147024872\ncp: cannot stat '/kaggle/working/content/usr/local/lib/python3.8/dist-packages': No such file or directory\nrm: cannot remove '/kaggle/working/content/usr': No such file or directory\nrm: cannot remove 'Dependencies_AUT.7z.001': No such file or directory\nrm: cannot remove 'Dependencies_AUT.7z.002': No such file or directory\nrm: cannot remove 'Dependencies_AUT.7z.003': No such file or directory\nrm: cannot remove 'Dependencies_AUT.7z.004': No such file or directory\nrm: cannot remove 'Dependencies_AUT.7z.005': No such file or directory\nrm: cannot remove 'Dependencies_AUT.7z.006': No such file or directory\n\u001b[33mWARNING: Skipping diffusers as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCloning into 'diffusers'...\nremote: Enumerating objects: 19100, done.\u001b[K\nremote: Counting objects: 100% (25/25), done.\u001b[K\nremote: Compressing objects: 100% (18/18), done.\u001b[K\nremote: Total 19100 (delta 10), reused 11 (delta 7), pack-reused 19075\u001b[K\nReceiving objects: 100% (19100/19100), 13.66 MiB | 7.88 MiB/s, done.\nResolving deltas: 100% (13761/13761), done.\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping xformers as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!sudo apt-get install git-lfs\n!git lfs install","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:25:02.21297Z","iopub.execute_input":"2023-05-26T14:25:02.213358Z","iopub.status.idle":"2023-05-26T14:25:05.767826Z","shell.execute_reply.started":"2023-05-26T14:25:02.213326Z","shell.execute_reply":"2023-05-26T14:25:05.766663Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ngit-lfs is already the newest version (3.0.2-1ubuntu0.2).\n0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\nGit LFS initialized.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport time\nfrom IPython.display import clear_output\nfrom IPython.utils import capture\nimport wget\n\n#@markdown - Skip this cell if you are loading a previous session\n\n#@markdown ---\n\nModel_Version = \"1.5\" #@param [ \"1.5\", \"V2-512px\", \"V2-768px\"]\n\n#@markdown - Choose which version to finetune.\n\n#@markdown ---\n\nwith capture.capture_output() as cap: \n  %cd /kaggle/working/content/\n\nHuggingface_Token = \"your_huggingface_token\" #@param {type:\"string\"}\ntoken=Huggingface_Token\n\n#@markdown - Leave EMPTY if you're using the v2 model.\n#@markdown - Make sure you've accepted the terms in https://huggingface.co/runwayml/stable-diffusion-v1-5\n\n#@markdown ---\n\nPath_to_HuggingFace= \"\" #@param {type:\"string\"}\n\n#@markdown - Load and finetune a model from Hugging Face, must specify if v2, use the format \"profile/model\" like : runwayml/stable-diffusion-v1-5\n\n#@markdown Or\n\nCKPT_Path = \"\" #@param {type:\"string\"}\n\n#@markdown Or\n\nCKPT_Link = \"\" #@param {type:\"string\"}\n\n#@markdown - A CKPT direct link, huggingface CKPT link or a shared CKPT from gdrive.\n#@markdown ---\n\nCompatibility_Mode=False #@param {type:\"boolean\"}\n#@markdown - Enable only if you're getting conversion errors.\n\n\ndef downloadmodel():\n  token=Huggingface_Token\n  if token==\"\":\n      token=input(\"Insert your huggingface token :\")\n  if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5'):\n    !rm -r /kaggle/working/content/stable-diffusion-v1-5\n  clear_output()\n\n  %cd /kaggle/working/content/\n  clear_output()\n  !mkdir /kaggle/working/content/stable-diffusion-v1-5\n  %cd /kaggle/working/content/stable-diffusion-v1-5\n  !git init\n  !git lfs install --system --skip-repo\n  !git remote add -f origin  \"https://USER:{token}@huggingface.co/runwayml/stable-diffusion-v1-5\"\n  !git config core.sparsecheckout true\n  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nmodel_index.json\" > .git/info/sparse-checkout\n  !git pull origin main\n  if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n    !git clone \"https://USER:{token}@huggingface.co/stabilityai/sd-vae-ft-mse\"\n    !mv /kaggle/working/content/stable-diffusion-v1-5/sd-vae-ft-mse /kaggle/working/content/stable-diffusion-v1-5/vae\n    !rm -r /kaggle/working/content/stable-diffusion-v1-5/.git\n    %cd /kaggle/working/content/stable-diffusion-v1-5\n    !rm model_index.json\n    time.sleep(1)    \n    wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n    !sed -i 's@\"clip_sample\": false@@g' /kaggle/working/content/stable-diffusion-v1-5/scheduler/scheduler_config.json\n    !sed -i 's@\"trained_betas\": null,@\"trained_betas\": null@g' /kaggle/working/content/stable-diffusion-v1-5/scheduler/scheduler_config.json\n    !sed -i 's@\"sample_size\": 256,@\"sample_size\": 512,@g' /kaggle/working/content/stable-diffusion-v1-5/vae/config.json  \n    %cd /kaggle/working/content/    \n    clear_output()\n    print('\u001b[1;32mDONE !')\n  else:\n    while not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n         print('\u001b[1;31mMake sure you accepted the terms in https://huggingface.co/runwayml/stable-diffusion-v1-5')\n         time.sleep(5)\n\n\ndef newdownloadmodel():\n\n  %cd /kaggle/working/content/\n  clear_output()\n  !mkdir /kaggle/working/content/stable-diffusion-v2-768\n  %cd /kaggle/working/content/stable-diffusion-v2-768\n  !git init\n  !git lfs install --system --skip-repo\n  !git remote add -f origin  \"https://USER:{token}@huggingface.co/stabilityai/stable-diffusion-2\"\n  !git config core.sparsecheckout true\n  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nmodel_index.json\" > .git/info/sparse-checkout\n  !git pull origin main\n  clear_output()\n  print('\u001b[1;32mDONE !')\n\n\ndef newdownloadmodelb():\n\n  %cd /kaggle/working/content/\n  clear_output()\n  !mkdir /kaggle/working/content/stable-diffusion-v2-512\n  %cd /kaggle/working/content/stable-diffusion-v2-512\n  !git init\n  !git lfs install --system --skip-repo\n  !git remote add -f origin  \"https://USER:{token}@huggingface.co/stabilityai/stable-diffusion-2-base\"\n  !git config core.sparsecheckout true\n  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nmodel_index.json\" > .git/info/sparse-checkout\n  !git pull origin main\n  clear_output()\n  print('\u001b[1;32mDONE !')\n    \n\nif Path_to_HuggingFace != \"\":\n  if V2_model:\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom'):\n      !rm -r /kaggle/working/content/stable-diffusion-custom\n    clear_output()\n    %cd /kaggle/working/content/\n    clear_output()\n    !mkdir /kaggle/working/content/stable-diffusion-custom\n    %cd /kaggle/working/content/stable-diffusion-custom\n    !git init\n    !git lfs install --system --skip-repo\n    !git remote add -f origin  \"https://USER:{token}@huggingface.co/{Path_to_HuggingFace}\"\n    !git config core.sparsecheckout true\n    !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nmodel_index.json\" > .git/info/sparse-checkout\n    !git pull origin main\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n      !rm -r /kaggle/working/content/stable-diffusion-custom/.git\n      %cd /kaggle/working/content/ \n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"   \n      clear_output()\n      print('\u001b[1;32mDONE !')\n    else:\n      while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n            print('\u001b[1;31mCheck the link you provided')\n            time.sleep(5)\n  else:\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom'):\n      !rm -r /kaggle/working/content/stable-diffusion-custom\n    clear_output()\n    %cd /kaggle/working/content/\n    clear_output()\n    !mkdir /kaggle/working/content/stable-diffusion-custom\n    %cd /kaggle/working/content/stable-diffusion-custom\n    !git init\n    !git lfs install --system --skip-repo\n    !git remote add -f origin  \"https://USER:{token}@huggingface.co/{Path_to_HuggingFace}\"\n    !git config core.sparsecheckout true\n    !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nmodel_index.json\" > .git/info/sparse-checkout\n    !git pull origin main\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n      !git clone \"https://USER:{token}@huggingface.co/stabilityai/sd-vae-ft-mse\"\n      !mv /kaggle/working/content/stable-diffusion-custom/sd-vae-ft-mse /kaggle/working/content/stable-diffusion-custom/vae\n      !rm -r /kaggle/working/content/stable-diffusion-custom/.git\n      %cd /kaggle/working/content/stable-diffusion-custom\n      !rm model_index.json\n      time.sleep(1)\n      wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n      !sed -i 's@\"clip_sample\": false@@g' /kaggle/working/content/stable-diffusion-custom/scheduler/scheduler_config.json\n      !sed -i 's@\"trained_betas\": null,@\"trained_betas\": null@g' /kaggle/working/content/stable-diffusion-custom/scheduler/scheduler_config.json\n      !sed -i 's@\"sample_size\": 256,@\"sample_size\": 512,@g' /kaggle/working/content/stable-diffusion-custom/vae/config.json    \n      %cd /kaggle/working/content/ \n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"   \n      clear_output()\n      print('\u001b[1;32mDONE !')\n    else:\n      while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n            print('\u001b[1;31mCheck the link you provided')\n            time.sleep(5)    \n\n\nelif CKPT_Path !=\"\":\n  if os.path.exists('/kaggle/working/content/stable-custom'):\n    !rm -r /kaggle/working/content/stable-diffusion-custom\n  if os.path.exists(str(CKPT_Path)):\n    !mkdir /kaggle/working/content/stable-diffusion-custom\n    with capture.capture_output() as cap:\n      if Compatibility_Mode:\n        !wget https://raw.githubusercontent.com/huggingface/diffusers/039958eae55ff0700cfb42a7e72739575ab341f1/scripts/convert_original_stable_diffusion_to_diffusers.py\n        !python /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$CKPT_Path\" --dump_path /kaggle/working/content/stable-diffusion-custom\n        !rm /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py\n      else:           \n        !python /kaggle/working/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$CKPT_Path\" --dump_path /kaggle/working/content/stable-diffusion-custom\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n      !rm /kaggle/working/content/v1-inference.yaml\n      clear_output()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"\n      print('\u001b[1;32mDONE !')\n    else:\n      !rm /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py\n      !rm /kaggle/working/content/v1-inference.yaml\n      !rm -r /kaggle/working/content/stable-diffusion-custom\n      while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n        print('\u001b[1;31mConversion error, Insufficient RAM or corrupt CKPT, use a 4GB CKPT instead of 7GB')\n        time.sleep(5)\n  else:\n    while not os.path.exists(str(CKPT_Path)):\n       print('\u001b[1;31mWrong path, use the colab file explorer to copy the path')\n       time.sleep(5)\n  \n\nelif CKPT_Link !=\"\":   \n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom'):\n      !rm -r /kaggle/working/content/stable-diffusion-custom   \n    !gdown --fuzzy -O model.ckpt $CKPT_Link\n    if os.path.exists('/kaggle/working/content/model.ckpt'):\n      if os.path.getsize(\"/kaggle/working/content/model.ckpt\") > 1810671599:\n        !mkdir /kaggle/working/content/stable-diffusion-custom\n        with capture.capture_output() as cap: \n          if Compatibility_Mode:\n            !wget https://raw.githubusercontent.com/huggingface/diffusers/039958eae55ff0700cfb42a7e72739575ab341f1/scripts/convert_original_stable_diffusion_to_diffusers.py\n            !python /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path /kaggle/working/content/model.ckpt --dump_path /kaggle/working/content/stable-diffusion-custom\n            !rm /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py            \n          else:           \n            !python /kaggle/working/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path /kaggle/working/content/model.ckpt --dump_path /kaggle/working/content/stable-diffusion-custom\n        if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n          clear_output()\n          MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"\n          print('\u001b[1;32mDONE !')\n          !rm /kaggle/working/content/v1-inference.yaml\n          !rm /kaggle/working/content/model.ckpt\n        else:\n          if os.path.exists('/kaggle/working/content/v1-inference.yaml'):\n            !rm /kaggle/working/content/v1-inference.yaml\n          !rm /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py\n          !rm -r /kaggle/working/content/stable-diffusion-custom\n          !rm /kaggle/working/content/model.ckpt\n          while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n            print('\u001b[1;31mConversion error, Insufficient RAM or corrupt CKPT, use a 4GB CKPT instead of 7GB')\n            time.sleep(5)\n      else:\n        while os.path.getsize('/kaggle/working/content/model.ckpt') < 1810671599:\n           print('\u001b[1;31mWrong link, check that the link is valid')\n           time.sleep(5)\n    \n\nelse:\n  if Model_Version==\"1.5\":\n    if not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5'):\n      downloadmodel()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v1-5\"\n    else:\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v1-5\"\n      print(\"\u001b[1;32mThe v1.5 model already exists, using this model.\")\n  elif Model_Version==\"V2-512px\":\n    if not os.path.exists('/kaggle/working/content/stable-diffusion-v2-512'):\n      newdownloadmodelb()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-512\"\n    else:\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-512\"\n      print(\"\u001b[1;32mThe v2-512px model already exists, using this model.\")      \n  elif Model_Version==\"V2-768px\":\n    if not os.path.exists('/kaggle/working/content/stable-diffusion-v2-768'):   \n      newdownloadmodel()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-768\"\n    else:\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-768\"\n      print(\"\u001b[1;32mThe v2-768px model already exists, using this model.\")","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:26:41.787873Z","iopub.execute_input":"2023-05-26T14:26:41.788454Z","iopub.status.idle":"2023-05-26T14:30:49.063719Z","shell.execute_reply.started":"2023-05-26T14:26:41.788419Z","shell.execute_reply":"2023-05-26T14:30:49.062549Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\u001b[1;32mDONE !\n","output_type":"stream"}]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:33:18.283508Z","iopub.execute_input":"2023-05-26T14:33:18.284651Z","iopub.status.idle":"2023-05-26T14:33:19.229492Z","shell.execute_reply.started":"2023-05-26T14:33:18.284606Z","shell.execute_reply":"2023-05-26T14:33:19.228209Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/models","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:33:23.147293Z","iopub.execute_input":"2023-05-26T14:33:23.147759Z","iopub.status.idle":"2023-05-26T14:33:24.087219Z","shell.execute_reply.started":"2023-05-26T14:33:23.147705Z","shell.execute_reply":"2023-05-26T14:33:24.085911Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:33:26.842554Z","iopub.execute_input":"2023-05-26T14:33:26.843349Z","iopub.status.idle":"2023-05-26T14:33:27.785347Z","shell.execute_reply.started":"2023-05-26T14:33:26.843312Z","shell.execute_reply":"2023-05-26T14:33:27.78405Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/instance_images","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:34:01.611265Z","iopub.execute_input":"2023-05-26T14:34:01.611677Z","iopub.status.idle":"2023-05-26T14:34:02.601858Z","shell.execute_reply.started":"2023-05-26T14:34:01.611641Z","shell.execute_reply":"2023-05-26T14:34:02.600655Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‘/kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/instance_images’: File exists\n","output_type":"stream"}]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/Regularization_images","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:39:53.886658Z","iopub.execute_input":"2023-05-26T14:39:53.887223Z","iopub.status.idle":"2023-05-26T14:39:54.911072Z","shell.execute_reply.started":"2023-05-26T14:39:53.887185Z","shell.execute_reply":"2023-05-26T14:39:54.909922Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‘/kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/Regularization_images’: File exists\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom IPython.display import clear_output\nfrom IPython.utils import capture\nfrom os import listdir\nfrom os.path import isfile\nimport wget\nimport time\n\n#@markdown #Create/Load a Session\n\ntry:\n  MODEL_NAME\n  pass\nexcept:\n  MODEL_NAME=\"\"\n  \nPT=\"\"\n\nSession_Name = \"your_instance_name\" #@param{type: 'string'}\nwhile Session_Name==\"\":\n  print('\u001b[1;31mInput the Session Name:') \n  Session_Name=input('')\nSession_Name=Session_Name.replace(\" \",\"_\")\n\n#@markdown - Enter the session name, it if it exists, it will load it, otherwise it'll create an new session.\n\nSession_Link_optional = \"\" #@param{type: 'string'}\n\n#@markdown - Import a session from another gdrive, the shared gdrive link must point to the specific session's folder that contains the trained CKPT, remove any intermediary CKPT if any.\n\nWORKSPACE='/kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth'\n\nif Session_Link_optional !=\"\":\n  print('\u001b[1;32mDownloading session...')\nwith capture.capture_output() as cap:\n  %cd /kaggle/working/content\n  if Session_Link_optional != \"\":\n    if not os.path.exists(str(WORKSPACE+'/Sessions')):\n      %mkdir -p $WORKSPACE'/Sessions'\n      time.sleep(1)\n    %cd $WORKSPACE'/Sessions'\n    !gdown --folder --remaining-ok -O $Session_Name  $Session_Link_optional\n    %cd $Session_Name\n    !rm -r instance_images\n    !rm -r Regularization_images\n    !unzip instance_images.zip\n    !mv *.ckpt $Session_Name\".ckpt\"\n    %cd /kaggle/working/content\n\n\nINSTANCE_NAME=Session_Name\nOUTPUT_DIR=\"/kaggle/working/content/models/\"+Session_Name\nSESSION_DIR=WORKSPACE+'/Sessions/'+Session_Name\nINSTANCE_DIR=SESSION_DIR+'/instance_images'\nMDLPTH=str(SESSION_DIR+\"/\"+Session_Name+'.ckpt')\nCLASS_DIR=SESSION_DIR+'/Regularization_images'\n\nContains_faces = \"No\" #@param [\"No\", \"Female\", \"Male\", \"Both\"]\n\n#@markdown - Keep it \"No\" if you're not familiar with it, as it can produce incoherent output (to be removed soon).\n\ndef reg():\n  with capture.capture_output() as cap:\n    if Contains_faces!=\"No\":\n      if not os.path.exists(str(CLASS_DIR)):\n        %mkdir -p \"$CLASS_DIR\"\n      %cd $CLASS_DIR\n      !rm -r Women Men Mix\n      !wget -O Womenz 'https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/Regularization/Women'\n      !wget -O Menz 'https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/Regularization/Men'\n      !wget -O Mixz 'https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/Regularization/Mix'\n      !unzip Menz\n      !unzip Womenz\n      !unzip Mixz\n      !rm Menz Womenz Mixz\n      !find . -name \"* *\" -type f | rename 's/ /_/g'\n      %cd /kaggle/working/content               \n\nV2=False\n\nif os.path.exists(str(SESSION_DIR+\"/\"+INSTANCE_NAME+\"/unet/diffusion_pytorch_model.bin\")):\n  print('\u001b[1;32mV2 Model found, Loading...')\n  reg()\n  if not os.path.exists(\"/kaggle/working/content/models/\"):\n    !mkdir \"/kaggle/working/content/models/\"\n  !cp -r $SESSION_DIR/$INSTANCE_NAME /kaggle/working/content/models/\n  resume=True\n  V2=True\n  print('\u001b[1;32mSession Loaded, proceed to the training cell')\n\n\nelif os.path.exists(str(SESSION_DIR)):\n  if not os.path.exists(MDLPTH) and '.ckpt' in str([ckpt for ckpt in listdir(SESSION_DIR) if ckpt.split(\".\")[-1]==\"ckpt\"]):  \n    \n    def f(n):  \n      k=0\n      for i in [ckpt for ckpt in listdir(SESSION_DIR) if ckpt.split(\".\")[-1]==\"ckpt\"]:    \n        if k==n:    \n          !mv $SESSION_DIR/$i $MDLPTH\n        k=k+1\n\n    k=0\n    print('\u001b[1;33mNo final checkpoint model found, select which intermediary checkpoint to use (000 to skip):\\n\u001b[1;34m')\n\n    for i in [ckpt for ckpt in listdir(SESSION_DIR) if ckpt.split(\".\")[-1]==\"ckpt\"]:    \n      print(str(k)+'- '+i)\n      k=k+1\n    n=input()\n    while int(n)>k-1:\n      n=input()  \n    if n!=\"000\":\n      f(int(n))\n      print('\u001b[1;32mUsing the model '+ i+\" ...\")\n      time.sleep(2)\n    else:\n      print('\u001b[1;32mSkipping the intermediary checkpoints.')\n    del n\n\nif not V2:\n  \n  if os.path.exists(str(SESSION_DIR)) and not os.path.exists(MDLPTH):\n    print('\u001b[1;32mLoading session with no previous model, using the original model or the custom downloaded model')\n    reg()\n    if MODEL_NAME==\"\":\n      print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n    else:\n      print('\u001b[1;32mSession Loaded, proceed to uploading instance images')\n\n  elif os.path.exists(MDLPTH):\n    print('\u001b[1;32mSession found, loading the trained model ...')\n    reg()\n    %mkdir -p \"$OUTPUT_DIR\"\n    !python /kaggle/working/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$MDLPTH\" --dump_path \"$OUTPUT_DIR\" --session_dir \"$SESSION_DIR\"\n    if os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n      resume=True    \n      !rm /kaggle/working/content/v1-inference.yaml\n      clear_output()\n      print('\u001b[1;32mSession loaded.')\n    else:     \n      !rm /kaggle/working/content/v1-inference.yaml\n      if not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n        print('\u001b[1;31mConversion error, if the error persists, remove the CKPT file from the current session folder')\n\n  elif not os.path.exists(str(SESSION_DIR)):\n      %mkdir -p \"$INSTANCE_DIR\"\n      print('\u001b[1;32mCreating session...')\n      reg()\n      if MODEL_NAME==\"\":\n        print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n      else:\n        print('\u001b[1;32mSession created, proceed to uploading instance images')\n\n    \nif Contains_faces == \"Female\":\n  CLASS_DIR=CLASS_DIR+'/Women'\nif Contains_faces == \"Male\":\n  CLASS_DIR=CLASS_DIR+'/Men'\nif Contains_faces == \"Both\":\n  CLASS_DIR=CLASS_DIR+'/Mix'\n\ntry:\n  Contain_f\n  del Contain_f\nexcept:\n  pass\n\n    #@markdown \n\n    #@markdown # The most importent step is to rename the instance pictures of each subject to a unique unknown identifier, example :\n    #@markdown - If you have 30 pictures of yourself, simply select them all and rename only one to the chosen identifier for example : phtmejhn, the files would be : phtmejhn (1).jpg, phtmejhn (2).png ....etc then upload them, do the same for other people or objects with a different identifier, and that's it.\n    #@markdown - Check out this example : https://i.imgur.com/d2lD3rz.jpeg","metadata":{"execution":{"iopub.status.busy":"2023-05-26T14:46:03.368492Z","iopub.execute_input":"2023-05-26T14:46:03.36911Z","iopub.status.idle":"2023-05-26T14:46:16.490475Z","shell.execute_reply.started":"2023-05-26T14:46:03.369072Z","shell.execute_reply":"2023-05-26T14:46:16.489307Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"\u001b[1;32mCreating session...\n\u001b[1;32mSession created, proceed to uploading instance images\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil","metadata":{"execution":{"iopub.status.busy":"2023-05-26T15:03:29.650225Z","iopub.execute_input":"2023-05-26T15:03:29.650681Z","iopub.status.idle":"2023-05-26T15:03:29.656472Z","shell.execute_reply.started":"2023-05-26T15:03:29.650646Z","shell.execute_reply":"2023-05-26T15:03:29.655307Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"from PIL import Image","metadata":{"execution":{"iopub.status.busy":"2023-05-26T15:03:34.19976Z","iopub.execute_input":"2023-05-26T15:03:34.20013Z","iopub.status.idle":"2023-05-26T15:03:34.205111Z","shell.execute_reply.started":"2023-05-26T15:03:34.2001Z","shell.execute_reply":"2023-05-26T15:03:34.20404Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n","metadata":{"execution":{"iopub.status.busy":"2023-05-26T15:03:41.35223Z","iopub.execute_input":"2023-05-26T15:03:41.352617Z","iopub.status.idle":"2023-05-26T15:03:41.357121Z","shell.execute_reply.started":"2023-05-26T15:03:41.352565Z","shell.execute_reply":"2023-05-26T15:03:41.356118Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/action /kaggle/working/my","metadata":{"execution":{"iopub.status.busy":"2023-05-26T15:04:13.480645Z","iopub.execute_input":"2023-05-26T15:04:13.481023Z","iopub.status.idle":"2023-05-26T15:04:14.925567Z","shell.execute_reply.started":"2023-05-26T15:04:13.480987Z","shell.execute_reply":"2023-05-26T15:04:14.924274Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"#@markdown #Instance Images\n#@markdown ----\n\n#@markdown\n#@markdown - Run the cell to Upload the instance pictures.\n\nRemove_existing_instance_images= True #@param{type: 'boolean'}\n#@markdown - Uncheck the box to keep the existing instance images.\n\n\nif Remove_existing_instance_images:\n  if os.path.exists(str(INSTANCE_DIR)):\n    !rm -r \"$INSTANCE_DIR\"\n\nif not os.path.exists(str(INSTANCE_DIR)):\n  %mkdir -p \"$INSTANCE_DIR\"\n\nIMAGES_FOLDER_OPTIONAL=\"/kaggle/working/my\" #@param{type: 'string'}\n\n#@markdown - If you prefer to specify directly the folder of the pictures instead of uploading, this will add the pictures to the existing (if any) instance images. Leave EMPTY to upload.\n\nCrop_images= True #@param{type: 'boolean'}\nCrop_size = \"512\" #@param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"]\nCrop_size=int(Crop_size)\n\n#@markdown - Unless you want to crop them manually in a precise way, you don't need to crop your instance images externally.\n\nwhile IMAGES_FOLDER_OPTIONAL !=\"\" and not os.path.exists(str(IMAGES_FOLDER_OPTIONAL)):\n  print('\u001b[1;31mThe image folder specified does not exist, use the colab file explorer to copy the path :')\n  IMAGES_FOLDER_OPTIONAL=input('')\n\nif IMAGES_FOLDER_OPTIONAL!=\"\":\n  if Crop_images:\n    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      extension = filename.split(\".\")[1]\n      identifier=filename.split(\".\")[0]\n      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n      file = Image.open(IMAGES_FOLDER_OPTIONAL+\"/\"+filename)\n      width, height = file.size\n      if file.size !=(Crop_size, Crop_size):      \n        side_length = min(width, height)\n        left = (width - side_length)/2\n        top = (height - side_length)/2\n        right = (width + side_length)/2\n        bottom = (height + side_length)/2\n        image = file.crop((left, top, right, bottom))\n        image = image.resize((Crop_size, Crop_size))\n        if (extension.upper() == \"JPG\"):\n            image.save(new_path_with_file, format=\"JPEG\", quality = 100)\n        else:\n            image.save(new_path_with_file, format=extension.upper())\n      else:\n        !cp \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n\n  else:\n    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      %cp -r \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n \n  print('\\n\u001b[1;32mDone, proceed to the training cell')\n\n\nelif IMAGES_FOLDER_OPTIONAL ==\"\":\n  uploaded = files.upload()\n  if Crop_images:\n    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      shutil.move(filename, INSTANCE_DIR)\n      extension = filename.split(\".\")[1]\n      identifier=filename.split(\".\")[0]\n      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n      file = Image.open(new_path_with_file)\n      width, height = file.size\n      if file.size !=(Crop_size, Crop_size):        \n        side_length = min(width, height)\n        left = (width - side_length)/2\n        top = (height - side_length)/2\n        right = (width + side_length)/2\n        bottom = (height + side_length)/2\n        image = file.crop((left, top, right, bottom))\n        image = image.resize((Crop_size, Crop_size))\n        if (extension.upper() == \"JPG\"):\n            image.save(new_path_with_file, format=\"JPEG\", quality = 100)\n        else:\n            image.save(new_path_with_file, format=extension.upper())\n      else:\n          image.save(new_path_with_file, format=extension.upper())\n      clear_output()\n  else:\n    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      shutil.move(filename, INSTANCE_DIR)\n      clear_output()\n\n  print('\\n\u001b[1;32mDone, proceed to the training cell')\n\nwith capture.capture_output() as cap:\n  %cd \"$INSTANCE_DIR\"\n  !find . -name \"* *\" -type f | rename 's/ /-/g'\n  %cd /kaggle/working/content\n  if os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n    %rm -r INSTANCE_DIR+\"/.ipynb_checkpoints\"    \n\n  %cd $SESSION_DIR\n  !rm instance_images.zip\n  !zip -r instance_images instance_images\n  %cd /kaggle/working/content","metadata":{"execution":{"iopub.status.busy":"2023-05-26T15:04:40.390044Z","iopub.execute_input":"2023-05-26T15:04:40.390448Z","iopub.status.idle":"2023-05-26T15:05:09.164514Z","shell.execute_reply.started":"2023-05-26T15:04:40.390411Z","shell.execute_reply":"2023-05-26T15:05:09.163236Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"  |███████████████| 30/30 Uploaded","output_type":"stream"},{"name":"stdout","text":"\n\u001b[1;32mDone, proceed to the training cell\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /","metadata":{"execution":{"iopub.status.busy":"2023-05-26T15:05:09.502785Z","iopub.execute_input":"2023-05-26T15:05:09.503177Z","iopub.status.idle":"2023-05-26T15:05:09.511216Z","shell.execute_reply.started":"2023-05-26T15:05:09.503143Z","shell.execute_reply":"2023-05-26T15:05:09.510268Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"/\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install diffusers\"[training]\" accelerate \"transformers>=4.4.2\"","metadata":{"execution":{"iopub.status.busy":"2023-05-26T15:06:03.916544Z","iopub.execute_input":"2023-05-26T15:06:03.917268Z","iopub.status.idle":"2023-05-26T15:06:16.903109Z","shell.execute_reply.started":"2023-05-26T15:06:03.917234Z","shell.execute_reply":"2023-05-26T15:06:16.901956Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Requirement already satisfied: diffusers[training] in /opt/conda/lib/python3.10/site-packages (0.9.0.dev0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.12.0)\nRequirement already satisfied: transformers>=4.4.2 in /opt/conda/lib/python3.10/site-packages (4.29.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (5.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (3.12.0)\nRequirement already satisfied: huggingface-hub>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (0.14.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (1.23.5)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (2.28.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (9.5.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (2.1.0)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (2.12.3)\nCollecting modelcards>=0.1.4 (from diffusers[training])\n  Downloading modelcards-0.1.6-py3-none-any.whl (12 kB)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.4.1)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0+cu118)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.4.2) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.4.2) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers[training]) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers[training]) (4.5.0)\nRequirement already satisfied: Jinja2 in /opt/conda/lib/python3.10/site-packages (from modelcards>=0.1.4->diffusers[training]) (3.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (3.1)\nRequirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (2.0.0)\nRequirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (3.25.0)\nRequirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (15.0.7)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->diffusers[training]) (10.0.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets->diffusers[training]) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->diffusers[training]) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->diffusers[training]) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->diffusers[training]) (0.70.14)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->diffusers[training]) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->diffusers[training]) (0.18.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers[training]) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers[training]) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers[training]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers[training]) (2023.5.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers[training]) (3.15.0)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (2.17.3)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (3.4.3)\nRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (59.8.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (0.7.0)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (2.3.4)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (0.40.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->diffusers[training]) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->diffusers[training]) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->diffusers[training]) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->diffusers[training]) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->diffusers[training]) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->diffusers[training]) (1.3.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->diffusers[training]) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->diffusers[training]) (0.2.7)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->diffusers[training]) (1.16.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->diffusers[training]) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->diffusers[training]) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->diffusers[training]) (2.1.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->diffusers[training]) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->diffusers[training]) (2023.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->diffusers[training]) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->diffusers[training]) (3.2.2)\nInstalling collected packages: modelcards\nSuccessfully installed modelcards-0.1.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install diffusers\"[training]\" accelerate \"transformers>=4.4.2\"","metadata":{"execution":{"iopub.status.busy":"2023-05-26T15:06:29.699518Z","iopub.execute_input":"2023-05-26T15:06:29.699951Z","iopub.status.idle":"2023-05-26T15:06:41.696978Z","shell.execute_reply.started":"2023-05-26T15:06:29.699917Z","shell.execute_reply":"2023-05-26T15:06:41.695656Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Requirement already satisfied: diffusers[training] in /opt/conda/lib/python3.10/site-packages (0.9.0.dev0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.12.0)\nRequirement already satisfied: transformers>=4.4.2 in /opt/conda/lib/python3.10/site-packages (4.29.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (5.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (3.12.0)\nRequirement already satisfied: huggingface-hub>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (0.14.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (1.23.5)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (2.28.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (9.5.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (2.1.0)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (2.12.3)\nRequirement already satisfied: modelcards>=0.1.4 in /opt/conda/lib/python3.10/site-packages (from diffusers[training]) (0.1.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.4.1)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0+cu118)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.4.2) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.4.2) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers[training]) (2023.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers[training]) (4.5.0)\nRequirement already satisfied: Jinja2 in /opt/conda/lib/python3.10/site-packages (from modelcards>=0.1.4->diffusers[training]) (3.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (3.1)\nRequirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->accelerate) (2.0.0)\nRequirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (3.25.0)\nRequirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (15.0.7)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->diffusers[training]) (10.0.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets->diffusers[training]) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->diffusers[training]) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->diffusers[training]) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->diffusers[training]) (0.70.14)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->diffusers[training]) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->diffusers[training]) (0.18.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers[training]) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers[training]) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers[training]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers[training]) (2023.5.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers[training]) (3.15.0)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (2.17.3)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (3.4.3)\nRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (59.8.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (0.7.0)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (2.3.4)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard->diffusers[training]) (0.40.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->diffusers[training]) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->diffusers[training]) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->diffusers[training]) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->diffusers[training]) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->diffusers[training]) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->diffusers[training]) (1.3.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->diffusers[training]) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->diffusers[training]) (0.2.7)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->diffusers[training]) (1.16.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->diffusers[training]) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->diffusers[training]) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->diffusers[training]) (2.1.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->diffusers[training]) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->diffusers[training]) (2023.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->diffusers[training]) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->diffusers[training]) (3.2.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install ftfy","metadata":{"execution":{"iopub.status.busy":"2023-05-26T15:06:47.233129Z","iopub.execute_input":"2023-05-26T15:06:47.23352Z","iopub.status.idle":"2023-05-26T15:06:58.533438Z","shell.execute_reply.started":"2023-05-26T15:06:47.233486Z","shell.execute_reply":"2023-05-26T15:06:58.532149Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Collecting ftfy\n  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.6)\nInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.1.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2023-05-26T15:07:02.214893Z","iopub.execute_input":"2023-05-26T15:07:02.215285Z","iopub.status.idle":"2023-05-26T15:07:18.056613Z","shell.execute_reply.started":"2023-05-26T15:07:02.215252Z","shell.execute_reply":"2023-05-26T15:07:18.055299Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.39.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/content/","metadata":{"execution":{"iopub.status.busy":"2023-05-26T15:07:34.951908Z","iopub.execute_input":"2023-05-26T15:07:34.952319Z","iopub.status.idle":"2023-05-26T15:07:34.960124Z","shell.execute_reply.started":"2023-05-26T15:07:34.952285Z","shell.execute_reply":"2023-05-26T15:07:34.959102Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"/kaggle/working/content\n","output_type":"stream"}]},{"cell_type":"code","source":"import ftfy","metadata":{"execution":{"iopub.status.busy":"2023-05-26T15:07:37.530321Z","iopub.execute_input":"2023-05-26T15:07:37.531254Z","iopub.status.idle":"2023-05-26T15:07:37.621975Z","shell.execute_reply.started":"2023-05-26T15:07:37.531217Z","shell.execute_reply":"2023-05-26T15:07:37.621008Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-05-26T15:11:42.742114Z","iopub.execute_input":"2023-05-26T15:11:42.742523Z","iopub.status.idle":"2023-05-26T17:50:45.358689Z","shell.execute_reply.started":"2023-05-26T15:11:42.742491Z","shell.execute_reply":"2023-05-26T17:50:45.352517Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"#@markdown ---\n#@markdown #Start DreamBooth\n#@markdown ---\nimport os\nfrom subprocess import getoutput\nfrom IPython.display import HTML\nfrom IPython.display import clear_output\nimport time\nimport random\n\nResume_Training = False #@param {type:\"boolean\"}\n\ntry:\n   resume\n   if resume and not Resume_Training:\n     print('\u001b[1;31mOverwrite your previously trained model ?, answering \"yes\" will train a new model, answering \"no\" will resume the training of the previous model?  yes or no ?\u001b[0m')\n     while True:\n        ansres=input('')\n        if ansres=='no':\n          Resume_Training = True\n          del ansres\n          break\n        elif ansres=='yes':\n          Resume_Training = False\n          resume= False\n          break\nexcept:\n  pass\n\nwhile not Resume_Training and MODEL_NAME==\"\":\n  print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n  time.sleep(5)\n\n#@markdown  - If you're not satisfied with the result, check this box, run again the cell and it will continue training the current model.\n\nMODELT_NAME=MODEL_NAME\n\nTraining_Steps=6000 #@param{type: 'number'}\n#@markdown - Total Steps = Number of Instance images * 200, if you use 30 images, use 6000 steps, if you're not satisfied with the result, resume training for another 500 steps, and so on ...\n\nSeed='' #@param{type: 'string'}\n\n#@markdown - Leave empty for a random seed.\n\nResolution = \"512\" #@param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"]\nRes=int(Resolution)\n\n#@markdown - Higher resolution = Higher quality, make sure the instance images are cropped to this selected size (or larger).\n\nfp16 = True #@param {type:\"boolean\"}\n\n#@markdown - Enable/disable half-precision, disabling it will double the training time and produce 4GB-5.2GB checkpoints.\n\n#GC= \"\"\n#if Resolution!=\"512\":\nGC= \"--gradient_checkpointing\"\n\nif Seed =='' or Seed=='0':\n  Seed=random.randint(1, 999999)\nelse:\n  Seed=int(Seed)\n\nif fp16:\n  prec=\"fp16\"\nelse:\n  prec=\"no\"\n\ns = getoutput('nvidia-smi')\nif 'A100' in s:\n  precision=\"no\"\n  GC= \"\"\nelse:\n  precision=prec\n\n\nif Resume_Training and os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n  MODELT_NAME=OUTPUT_DIR\n  print('\u001b[1;32mResuming Training...\u001b[0m')\nelif Resume_Training and not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n  print('\u001b[1;31mPrevious model not found, training a new model...\u001b[0m')\n  MODELT_NAME=MODEL_NAME\n  while MODEL_NAME==\"\":\n    print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n    time.sleep(5)\n\nif os.path.getsize(MODELT_NAME+\"/text_encoder/pytorch_model.bin\") > 670901463:\n  V2=True\n\n#@markdown ---------------------------\n\ntry:\n   Contain_f\n   pass\nexcept:\n   Contain_f=Contains_faces\n\nEnable_text_encoder_training= True #@param{type: 'boolean'}\n\n#@markdown - At least 10% of the total training steps are needed, it doesn't matter if they are at the beginning or in the middle or the end, in case you're training the model multiple times.\n#@markdown - For example you can devide 5%, 5%, 5% on 3 training runs on the model, or 0%, 0%, 15%, given that 15% will cover the total training steps count (15% of 200 steps is not enough).\n\n#@markdown - Enter the % of the total steps for which to train the text_encoder\nTrain_text_encoder_for=100 #@param{type: 'number'}\n\n#@markdown - If you're training a style, keep it between 10-20%, if you're training on a person, set it between 50-70%, reduce it if you can't stylize the person/object.\n#@markdown - Higher % will give more weight to the instance, it gives stronger results at lower steps count, but harder to stylize.\n\nif Train_text_encoder_for>=100:\n  stptxt=Training_Steps\nelif Train_text_encoder_for==0:\n  Enable_text_encoder_training= False\n  stptxt=10\nelse:\n  stptxt=int((Training_Steps*Train_text_encoder_for)/100)\n\nif not Enable_text_encoder_training:\n  Contains_faces=\"No\"\nelse:\n   Contains_faces=Contain_f\n\nif Enable_text_encoder_training:\n  Textenc=\"--train_text_encoder\"\nelse:\n  Textenc=\"\"\n\n#@markdown ---------------------------\nSave_Checkpoint_Every_n_Steps = False #@param {type:\"boolean\"}\nSave_Checkpoint_Every=500 #@param{type: 'number'}\nif Save_Checkpoint_Every==None:\n  Save_Checkpoint_Every=1\n#@markdown - Minimum 200 steps between each save.\nstp=0\nStart_saving_from_the_step=500 #@param{type: 'number'}\nif Start_saving_from_the_step==None:\n  Start_saving_from_the_step=0\nif (Start_saving_from_the_step < 200):\n  Start_saving_from_the_step=Save_Checkpoint_Every\nstpsv=Start_saving_from_the_step\nif Save_Checkpoint_Every_n_Steps:\n  stp=Save_Checkpoint_Every\n#@markdown - Start saving intermediary checkpoints from this step.\n\nDisconnect_after_training=False #@param {type:\"boolean\"}\n\n#@markdown - Auto-disconnect from google colab after the training to avoid wasting compute units.\n\ndef txtenc_train(MODELT_NAME, INSTANCE_DIR, CLASS_DIR, OUTPUT_DIR, PT, Seed, precision, GC, Training_Steps):\n    print('\u001b[1;33mTraining the text encoder with regularization...\u001b[0m')\n    !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    --image_captions_filename \\\n    --train_text_encoder \\\n    --dump_only_text_encoder \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --class_data_dir=\"$CLASS_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --with_prior_preservation --prior_loss_weight=1.0 \\\n    --instance_prompt=\"$PT\"\\\n    --seed=$Seed \\\n    --resolution=512 \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 $GC \\\n    --use_8bit_adam \\\n    --learning_rate=2e-6 \\\n    --lr_scheduler=\"polynomial\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps \\\n    --num_class_images=200\n\ndef unet_train(SESSION_DIR, stpsv, stp, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, GC, Training_Steps):\n    clear_output()\n    print('\u001b[1;33mTraining the UNet...\u001b[0m')\n    !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    --image_captions_filename \\\n    --train_only_unet \\\n    --Session_dir=$SESSION_DIR \\\n    --save_starting_step=$stpsv \\\n    --save_n_steps=$stp \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --instance_prompt=\"$PT\" \\\n    --seed=$Seed \\\n    --resolution=$Res \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 $GC \\\n    --use_8bit_adam \\\n    --learning_rate=2e-6 \\\n    --lr_scheduler=\"polynomial\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps\n\n\ndef train_only_textenc(MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps):\n    print('\u001b[1;32mV2 + Standard GPU detected.\u001b[0m')\n    print('\u001b[1;33mTraining the text encoder...\u001b[0m')\n    !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    --image_captions_filename \\\n    --train_text_encoder \\\n    --dump_only_text_encoder \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --instance_prompt=\"$PT\" \\\n    --seed=$Seed \\\n    --resolution=512 \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 --gradient_checkpointing \\\n    --use_8bit_adam \\\n    --learning_rate=2e-6 \\\n    --lr_scheduler=\"polynomial\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps\n\ndef train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps):\n    clear_output()\n    print('\u001b[1;33mTraining the UNet...\u001b[0m')\n    !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    --image_captions_filename \\\n    --train_only_unet \\\n    --save_starting_step=$stpsv \\\n    --save_n_steps=$stp \\\n    --Session_dir=$SESSION_DIR \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --instance_prompt=\"$PT\" \\\n    --seed=$Seed \\\n    --resolution=$Res \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 --gradient_checkpointing \\\n    --use_8bit_adam \\\n    --learning_rate=2e-6 \\\n    --lr_scheduler=\"polynomial\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps\n\nif Contains_faces!=\"No\":  \n  if Enable_text_encoder_training :\n    txtenc_train(MODELT_NAME, INSTANCE_DIR, CLASS_DIR, OUTPUT_DIR, PT, Seed, precision, GC, Training_Steps=stptxt)\n  unet_train(SESSION_DIR, stpsv, stp, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, GC, Training_Steps)\n\nelif V2 and Resolution!=\"512\" and not(\"A100\" in s):\n    if Enable_text_encoder_training :\n      train_only_textenc(MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps=stptxt)\n    train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps)\n    \n\nelse:\n  !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    $Textenc \\\n    --image_captions_filename \\\n    --save_starting_step=$stpsv \\\n    --stop_text_encoder_training=$stptxt \\\n    --save_n_steps=$stp \\\n    --Session_dir=$SESSION_DIR \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --instance_prompt=\"$PT\" \\\n    --seed=$Seed \\\n    --resolution=$Res \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 $GC \\\n    --use_8bit_adam \\\n    --learning_rate=2e-6 \\\n    --lr_scheduler=\"polynomial\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps\n\nimport shutil\nshutil.rmtree(\"/kaggle/working/content/stable-diffusion-v1-5\")\n\nif os.path.exists('/kaggle/working/content/models/'+INSTANCE_NAME+'/unet/diffusion_pytorch_model.bin'):\n  %cd /kaggle/working/content    \n  !wget -O convertosd.py https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/convertosd.py\n  clear_output()\n  if precision==\"no\":\n    !sed -i '226s@.*@@' /kaggle/working/content/convertosd.py\n  !sed -i '201s@.*@    model_path = \"{OUTPUT_DIR}\"@' /kaggle/working/content/convertosd.py\n  !sed -i '202s@.*@    checkpoint_path= \"{SESSION_DIR}/{Session_Name}.ckpt\"@' /kaggle/working/content/convertosd.py\n  !python /kaggle/working/content/convertosd.py\n  clear_output()\n  if V2:\n    print(\"\u001b[1;32mSaving the diffusers model to your gdrive...\")\n    !cp -r $OUTPUT_DIR $SESSION_DIR\n  if os.path.exists(SESSION_DIR+\"/\"+INSTANCE_NAME+'.ckpt'):\n    if not os.path.exists(str(SESSION_DIR+'/tokenizer')) and not V2:\n      !cp -R '/kaggle/working/content/models/'$INSTANCE_NAME'/tokenizer' \"$SESSION_DIR\"\n    print(\"\u001b[1;32mDONE, the CKPT model is in your Gdrive in the sessions folder\")\n    if Disconnect_after_training :\n      runtime.unassign()\n  else:\n    print(\"\u001b[1;31mSomething went wrong\")\n    \nelse:\n  print(\"\u001b[1;31mSomething went wrong\")","metadata":{"execution":{"iopub.status.busy":"2023-05-26T15:11:42.742114Z","iopub.execute_input":"2023-05-26T15:11:42.742523Z","iopub.status.idle":"2023-05-26T17:50:45.358689Z","shell.execute_reply.started":"2023-05-26T15:11:42.742491Z","shell.execute_reply":"2023-05-26T17:50:45.352517Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"cp /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/your_instance_name/your_instance_name.ckpt /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-05-26T18:09:20.007078Z","iopub.execute_input":"2023-05-26T18:09:20.007492Z","iopub.status.idle":"2023-05-26T18:09:23.432654Z","shell.execute_reply.started":"2023-05-26T18:09:20.007457Z","shell.execute_reply":"2023-05-26T18:09:23.431161Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"import os","metadata":{"execution":{"iopub.status.busy":"2023-05-26T18:09:27.596562Z","iopub.execute_input":"2023-05-26T18:09:27.597273Z","iopub.status.idle":"2023-05-26T18:09:27.603494Z","shell.execute_reply.started":"2023-05-26T18:09:27.597233Z","shell.execute_reply":"2023-05-26T18:09:27.602218Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-05-26T18:09:30.036787Z","iopub.execute_input":"2023-05-26T18:09:30.037737Z","iopub.status.idle":"2023-05-26T18:09:30.04463Z","shell.execute_reply.started":"2023-05-26T18:09:30.0377Z","shell.execute_reply":"2023-05-26T18:09:30.043559Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'actionf.ckpt')","metadata":{"execution":{"iopub.status.busy":"2023-05-26T18:09:40.869651Z","iopub.execute_input":"2023-05-26T18:09:40.87004Z","iopub.status.idle":"2023-05-26T18:09:40.877321Z","shell.execute_reply.started":"2023-05-26T18:09:40.870007Z","shell.execute_reply":"2023-05-26T18:09:40.876404Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/actionf.ckpt","text/html":"<a href='actionf.ckpt' target='_blank'>actionf.ckpt</a><br>"},"metadata":{}}]}]}