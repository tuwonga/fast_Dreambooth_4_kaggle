{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/robertodefano/fast-dreambooth-sdv1-5-model?scriptVersionId=131138273\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive/MyDrive","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working/content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow-io-gcs-filesystem==0.31.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install google-cloud-bigquery-storage","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wget","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@markdown # Dependencies\nfrom subprocess import getoutput\nimport time\n\n%cd /kaggle/working/content/\n!pip install -q accelerate==0.12.0\nfor i in range(1,7):\n    !wget \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dependencies/Dependencies_AUT.{i}\"\n    !mv \"Dependencies_AUT.{i}\" \"Dependencies_AUT.7z.00{i}\"\n!7z x Dependencies_AUT.7z.001\ntime.sleep(2)\n!cp -r /kaggle/working/content/usr/local/lib/python3.8/dist-packages /usr/local/lib/python3.8/\n!rm -r /kaggle/working/content/usr\nfor i in range(1,7):\n    !rm \"Dependencies_AUT.7z.00{i}\"\n!pip uninstall -y diffusers\n!git clone --branch updt https://github.com/TheLastBen/diffusers\n!pip install -q /kaggle/working/content/diffusers\n!pip install -q -U pillow\ns = getoutput('nvidia-smi')\nif \"A100\" in s:\n    !wget https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/A100\n    %cd /kaggle/working/usr/local/lib/python3.8/dist-packages/xformers\n    !7z x -y /kaggle/working/content/A100\n    !rm /kaggle/working/content/A100\nif not (\"T4\" in s or \"A100\" in s):\n    !pip uninstall -q -y xformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get install git-lfs\n!git lfs install","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nfrom IPython.display import clear_output\nfrom IPython.utils import capture\nimport wget\n\n#@markdown - Skip this cell if you are loading a previous session\n\n#@markdown ---\n\nModel_Version = \"1.5\" #@param [ \"1.5\", \"V2-512px\", \"V2-768px\"]\n\n#@markdown - Choose which version to finetune.\n\n#@markdown ---\n\nwith capture.capture_output() as cap: \n  %cd /kaggle/working/content/\n\nHuggingface_Token = \"your_huggingface_token\" #@param {type:\"string\"}\ntoken=Huggingface_Token\n\n#@markdown - Leave EMPTY if you're using the v2 model.\n#@markdown - Make sure you've accepted the terms in https://huggingface.co/runwayml/stable-diffusion-v1-5\n\n#@markdown ---\n\nPath_to_HuggingFace= \"\" #@param {type:\"string\"}\n\n#@markdown - Load and finetune a model from Hugging Face, must specify if v2, use the format \"profile/model\" like : runwayml/stable-diffusion-v1-5\n\n#@markdown Or\n\nCKPT_Path = \"\" #@param {type:\"string\"}\n\n#@markdown Or\n\nCKPT_Link = \"\" #@param {type:\"string\"}\n\n#@markdown - A CKPT direct link, huggingface CKPT link or a shared CKPT from gdrive.\n#@markdown ---\n\nCompatibility_Mode=False #@param {type:\"boolean\"}\n#@markdown - Enable only if you're getting conversion errors.\n\n\ndef downloadmodel():\n  token=Huggingface_Token\n  if token==\"\":\n      token=input(\"Insert your huggingface token :\")\n  if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5'):\n    !rm -r /kaggle/working/content/stable-diffusion-v1-5\n  clear_output()\n\n  %cd /kaggle/working/content/\n  clear_output()\n  !mkdir /kaggle/working/content/stable-diffusion-v1-5\n  %cd /kaggle/working/content/stable-diffusion-v1-5\n  !git init\n  !git lfs install --system --skip-repo\n  !git remote add -f origin  \"https://USER:{token}@huggingface.co/runwayml/stable-diffusion-v1-5\"\n  !git config core.sparsecheckout true\n  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nmodel_index.json\" > .git/info/sparse-checkout\n  !git pull origin main\n  if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n    !git clone \"https://USER:{token}@huggingface.co/stabilityai/sd-vae-ft-mse\"\n    !mv /kaggle/working/content/stable-diffusion-v1-5/sd-vae-ft-mse /kaggle/working/content/stable-diffusion-v1-5/vae\n    !rm -r /kaggle/working/content/stable-diffusion-v1-5/.git\n    %cd /kaggle/working/content/stable-diffusion-v1-5\n    !rm model_index.json\n    time.sleep(1)    \n    wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n    !sed -i 's@\"clip_sample\": false@@g' /kaggle/working/content/stable-diffusion-v1-5/scheduler/scheduler_config.json\n    !sed -i 's@\"trained_betas\": null,@\"trained_betas\": null@g' /kaggle/working/content/stable-diffusion-v1-5/scheduler/scheduler_config.json\n    !sed -i 's@\"sample_size\": 256,@\"sample_size\": 512,@g' /kaggle/working/content/stable-diffusion-v1-5/vae/config.json  \n    %cd /kaggle/working/content/    \n    clear_output()\n    print('\u001b[1;32mDONE !')\n  else:\n    while not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n         print('\u001b[1;31mMake sure you accepted the terms in https://huggingface.co/runwayml/stable-diffusion-v1-5')\n         time.sleep(5)\n\n\ndef newdownloadmodel():\n\n  %cd /kaggle/working/content/\n  clear_output()\n  !mkdir /kaggle/working/content/stable-diffusion-v2-768\n  %cd /kaggle/working/content/stable-diffusion-v2-768\n  !git init\n  !git lfs install --system --skip-repo\n  !git remote add -f origin  \"https://USER:{token}@huggingface.co/stabilityai/stable-diffusion-2\"\n  !git config core.sparsecheckout true\n  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nmodel_index.json\" > .git/info/sparse-checkout\n  !git pull origin main\n  clear_output()\n  print('\u001b[1;32mDONE !')\n\n\ndef newdownloadmodelb():\n\n  %cd /kaggle/working/content/\n  clear_output()\n  !mkdir /kaggle/working/content/stable-diffusion-v2-512\n  %cd /kaggle/working/content/stable-diffusion-v2-512\n  !git init\n  !git lfs install --system --skip-repo\n  !git remote add -f origin  \"https://USER:{token}@huggingface.co/stabilityai/stable-diffusion-2-base\"\n  !git config core.sparsecheckout true\n  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nmodel_index.json\" > .git/info/sparse-checkout\n  !git pull origin main\n  clear_output()\n  print('\u001b[1;32mDONE !')\n    \n\nif Path_to_HuggingFace != \"\":\n  if V2_model:\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom'):\n      !rm -r /kaggle/working/content/stable-diffusion-custom\n    clear_output()\n    %cd /kaggle/working/content/\n    clear_output()\n    !mkdir /kaggle/working/content/stable-diffusion-custom\n    %cd /kaggle/working/content/stable-diffusion-custom\n    !git init\n    !git lfs install --system --skip-repo\n    !git remote add -f origin  \"https://USER:{token}@huggingface.co/{Path_to_HuggingFace}\"\n    !git config core.sparsecheckout true\n    !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nmodel_index.json\" > .git/info/sparse-checkout\n    !git pull origin main\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n      !rm -r /kaggle/working/content/stable-diffusion-custom/.git\n      %cd /kaggle/working/content/ \n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"   \n      clear_output()\n      print('\u001b[1;32mDONE !')\n    else:\n      while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n            print('\u001b[1;31mCheck the link you provided')\n            time.sleep(5)\n  else:\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom'):\n      !rm -r /kaggle/working/content/stable-diffusion-custom\n    clear_output()\n    %cd /kaggle/working/content/\n    clear_output()\n    !mkdir /kaggle/working/content/stable-diffusion-custom\n    %cd /kaggle/working/content/stable-diffusion-custom\n    !git init\n    !git lfs install --system --skip-repo\n    !git remote add -f origin  \"https://USER:{token}@huggingface.co/{Path_to_HuggingFace}\"\n    !git config core.sparsecheckout true\n    !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nmodel_index.json\" > .git/info/sparse-checkout\n    !git pull origin main\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n      !git clone \"https://USER:{token}@huggingface.co/stabilityai/sd-vae-ft-mse\"\n      !mv /kaggle/working/content/stable-diffusion-custom/sd-vae-ft-mse /kaggle/working/content/stable-diffusion-custom/vae\n      !rm -r /kaggle/working/content/stable-diffusion-custom/.git\n      %cd /kaggle/working/content/stable-diffusion-custom\n      !rm model_index.json\n      time.sleep(1)\n      wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n      !sed -i 's@\"clip_sample\": false@@g' /kaggle/working/content/stable-diffusion-custom/scheduler/scheduler_config.json\n      !sed -i 's@\"trained_betas\": null,@\"trained_betas\": null@g' /kaggle/working/content/stable-diffusion-custom/scheduler/scheduler_config.json\n      !sed -i 's@\"sample_size\": 256,@\"sample_size\": 512,@g' /kaggle/working/content/stable-diffusion-custom/vae/config.json    \n      %cd /kaggle/working/content/ \n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"   \n      clear_output()\n      print('\u001b[1;32mDONE !')\n    else:\n      while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n            print('\u001b[1;31mCheck the link you provided')\n            time.sleep(5)    \n\n\nelif CKPT_Path !=\"\":\n  if os.path.exists('/kaggle/working/content/stable-custom'):\n    !rm -r /kaggle/working/content/stable-diffusion-custom\n  if os.path.exists(str(CKPT_Path)):\n    !mkdir /kaggle/working/content/stable-diffusion-custom\n    with capture.capture_output() as cap:\n      if Compatibility_Mode:\n        !wget https://raw.githubusercontent.com/huggingface/diffusers/039958eae55ff0700cfb42a7e72739575ab341f1/scripts/convert_original_stable_diffusion_to_diffusers.py\n        !python /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$CKPT_Path\" --dump_path /kaggle/working/content/stable-diffusion-custom\n        !rm /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py\n      else:           \n        !python /kaggle/working/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$CKPT_Path\" --dump_path /kaggle/working/content/stable-diffusion-custom\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n      !rm /kaggle/working/content/v1-inference.yaml\n      clear_output()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"\n      print('\u001b[1;32mDONE !')\n    else:\n      !rm /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py\n      !rm /kaggle/working/content/v1-inference.yaml\n      !rm -r /kaggle/working/content/stable-diffusion-custom\n      while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n        print('\u001b[1;31mConversion error, Insufficient RAM or corrupt CKPT, use a 4GB CKPT instead of 7GB')\n        time.sleep(5)\n  else:\n    while not os.path.exists(str(CKPT_Path)):\n       print('\u001b[1;31mWrong path, use the colab file explorer to copy the path')\n       time.sleep(5)\n  \n\nelif CKPT_Link !=\"\":   \n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom'):\n      !rm -r /kaggle/working/content/stable-diffusion-custom   \n    !gdown --fuzzy -O model.ckpt $CKPT_Link\n    if os.path.exists('/kaggle/working/content/model.ckpt'):\n      if os.path.getsize(\"/kaggle/working/content/model.ckpt\") > 1810671599:\n        !mkdir /kaggle/working/content/stable-diffusion-custom\n        with capture.capture_output() as cap: \n          if Compatibility_Mode:\n            !wget https://raw.githubusercontent.com/huggingface/diffusers/039958eae55ff0700cfb42a7e72739575ab341f1/scripts/convert_original_stable_diffusion_to_diffusers.py\n            !python /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path /kaggle/working/content/model.ckpt --dump_path /kaggle/working/content/stable-diffusion-custom\n            !rm /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py            \n          else:           \n            !python /kaggle/working/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path /kaggle/working/content/model.ckpt --dump_path /kaggle/working/content/stable-diffusion-custom\n        if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n          clear_output()\n          MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"\n          print('\u001b[1;32mDONE !')\n          !rm /kaggle/working/content/v1-inference.yaml\n          !rm /kaggle/working/content/model.ckpt\n        else:\n          if os.path.exists('/kaggle/working/content/v1-inference.yaml'):\n            !rm /kaggle/working/content/v1-inference.yaml\n          !rm /kaggle/working/content/convert_original_stable_diffusion_to_diffusers.py\n          !rm -r /kaggle/working/content/stable-diffusion-custom\n          !rm /kaggle/working/content/model.ckpt\n          while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n            print('\u001b[1;31mConversion error, Insufficient RAM or corrupt CKPT, use a 4GB CKPT instead of 7GB')\n            time.sleep(5)\n      else:\n        while os.path.getsize('/kaggle/working/content/model.ckpt') < 1810671599:\n           print('\u001b[1;31mWrong link, check that the link is valid')\n           time.sleep(5)\n    \n\nelse:\n  if Model_Version==\"1.5\":\n    if not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5'):\n      downloadmodel()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v1-5\"\n    else:\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v1-5\"\n      print(\"\u001b[1;32mThe v1.5 model already exists, using this model.\")\n  elif Model_Version==\"V2-512px\":\n    if not os.path.exists('/kaggle/working/content/stable-diffusion-v2-512'):\n      newdownloadmodelb()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-512\"\n    else:\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-512\"\n      print(\"\u001b[1;32mThe v2-512px model already exists, using this model.\")      \n  elif Model_Version==\"V2-768px\":\n    if not os.path.exists('/kaggle/working/content/stable-diffusion-v2-768'):   \n      newdownloadmodel()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-768\"\n    else:\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-768\"\n      print(\"\u001b[1;32mThe v2-768px model already exists, using this model.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/instance_images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/Regularization_images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom IPython.display import clear_output\nfrom IPython.utils import capture\nfrom os import listdir\nfrom os.path import isfile\nimport wget\nimport time\n\n#@markdown #Create/Load a Session\n\ntry:\n  MODEL_NAME\n  pass\nexcept:\n  MODEL_NAME=\"\"\n  \nPT=\"\"\n\nSession_Name = \"your_instance_name\" #@param{type: 'string'}\nwhile Session_Name==\"\":\n  print('\u001b[1;31mInput the Session Name:') \n  Session_Name=input('')\nSession_Name=Session_Name.replace(\" \",\"_\")\n\n#@markdown - Enter the session name, it if it exists, it will load it, otherwise it'll create an new session.\n\nSession_Link_optional = \"\" #@param{type: 'string'}\n\n#@markdown - Import a session from another gdrive, the shared gdrive link must point to the specific session's folder that contains the trained CKPT, remove any intermediary CKPT if any.\n\nWORKSPACE='/kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth'\n\nif Session_Link_optional !=\"\":\n  print('\u001b[1;32mDownloading session...')\nwith capture.capture_output() as cap:\n  %cd /kaggle/working/content\n  if Session_Link_optional != \"\":\n    if not os.path.exists(str(WORKSPACE+'/Sessions')):\n      %mkdir -p $WORKSPACE'/Sessions'\n      time.sleep(1)\n    %cd $WORKSPACE'/Sessions'\n    !gdown --folder --remaining-ok -O $Session_Name  $Session_Link_optional\n    %cd $Session_Name\n    !rm -r instance_images\n    !rm -r Regularization_images\n    !unzip instance_images.zip\n    !mv *.ckpt $Session_Name\".ckpt\"\n    %cd /kaggle/working/content\n\n\nINSTANCE_NAME=Session_Name\nOUTPUT_DIR=\"/kaggle/working/content/models/\"+Session_Name\nSESSION_DIR=WORKSPACE+'/Sessions/'+Session_Name\nINSTANCE_DIR=SESSION_DIR+'/instance_images'\nMDLPTH=str(SESSION_DIR+\"/\"+Session_Name+'.ckpt')\nCLASS_DIR=SESSION_DIR+'/Regularization_images'\n\nContains_faces = \"No\" #@param [\"No\", \"Female\", \"Male\", \"Both\"]\n\n#@markdown - Keep it \"No\" if you're not familiar with it, as it can produce incoherent output (to be removed soon).\n\ndef reg():\n  with capture.capture_output() as cap:\n    if Contains_faces!=\"No\":\n      if not os.path.exists(str(CLASS_DIR)):\n        %mkdir -p \"$CLASS_DIR\"\n      %cd $CLASS_DIR\n      !rm -r Women Men Mix\n      !wget -O Womenz 'https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/Regularization/Women'\n      !wget -O Menz 'https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/Regularization/Men'\n      !wget -O Mixz 'https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/Regularization/Mix'\n      !unzip Menz\n      !unzip Womenz\n      !unzip Mixz\n      !rm Menz Womenz Mixz\n      !find . -name \"* *\" -type f | rename 's/ /_/g'\n      %cd /kaggle/working/content               \n\nV2=False\n\nif os.path.exists(str(SESSION_DIR+\"/\"+INSTANCE_NAME+\"/unet/diffusion_pytorch_model.bin\")):\n  print('\u001b[1;32mV2 Model found, Loading...')\n  reg()\n  if not os.path.exists(\"/kaggle/working/content/models/\"):\n    !mkdir \"/kaggle/working/content/models/\"\n  !cp -r $SESSION_DIR/$INSTANCE_NAME /kaggle/working/content/models/\n  resume=True\n  V2=True\n  print('\u001b[1;32mSession Loaded, proceed to the training cell')\n\n\nelif os.path.exists(str(SESSION_DIR)):\n  if not os.path.exists(MDLPTH) and '.ckpt' in str([ckpt for ckpt in listdir(SESSION_DIR) if ckpt.split(\".\")[-1]==\"ckpt\"]):  \n    \n    def f(n):  \n      k=0\n      for i in [ckpt for ckpt in listdir(SESSION_DIR) if ckpt.split(\".\")[-1]==\"ckpt\"]:    \n        if k==n:    \n          !mv $SESSION_DIR/$i $MDLPTH\n        k=k+1\n\n    k=0\n    print('\u001b[1;33mNo final checkpoint model found, select which intermediary checkpoint to use (000 to skip):\\n\u001b[1;34m')\n\n    for i in [ckpt for ckpt in listdir(SESSION_DIR) if ckpt.split(\".\")[-1]==\"ckpt\"]:    \n      print(str(k)+'- '+i)\n      k=k+1\n    n=input()\n    while int(n)>k-1:\n      n=input()  \n    if n!=\"000\":\n      f(int(n))\n      print('\u001b[1;32mUsing the model '+ i+\" ...\")\n      time.sleep(2)\n    else:\n      print('\u001b[1;32mSkipping the intermediary checkpoints.')\n    del n\n\nif not V2:\n  \n  if os.path.exists(str(SESSION_DIR)) and not os.path.exists(MDLPTH):\n    print('\u001b[1;32mLoading session with no previous model, using the original model or the custom downloaded model')\n    reg()\n    if MODEL_NAME==\"\":\n      print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n    else:\n      print('\u001b[1;32mSession Loaded, proceed to uploading instance images')\n\n  elif os.path.exists(MDLPTH):\n    print('\u001b[1;32mSession found, loading the trained model ...')\n    reg()\n    %mkdir -p \"$OUTPUT_DIR\"\n    !python /kaggle/working/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$MDLPTH\" --dump_path \"$OUTPUT_DIR\" --session_dir \"$SESSION_DIR\"\n    if os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n      resume=True    \n      !rm /kaggle/working/content/v1-inference.yaml\n      clear_output()\n      print('\u001b[1;32mSession loaded.')\n    else:     \n      !rm /kaggle/working/content/v1-inference.yaml\n      if not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n        print('\u001b[1;31mConversion error, if the error persists, remove the CKPT file from the current session folder')\n\n  elif not os.path.exists(str(SESSION_DIR)):\n      %mkdir -p \"$INSTANCE_DIR\"\n      print('\u001b[1;32mCreating session...')\n      reg()\n      if MODEL_NAME==\"\":\n        print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n      else:\n        print('\u001b[1;32mSession created, proceed to uploading instance images')\n\n    \nif Contains_faces == \"Female\":\n  CLASS_DIR=CLASS_DIR+'/Women'\nif Contains_faces == \"Male\":\n  CLASS_DIR=CLASS_DIR+'/Men'\nif Contains_faces == \"Both\":\n  CLASS_DIR=CLASS_DIR+'/Mix'\n\ntry:\n  Contain_f\n  del Contain_f\nexcept:\n  pass\n\n    #@markdown \n\n    #@markdown # The most importent step is to rename the instance pictures of each subject to a unique unknown identifier, example :\n    #@markdown - If you have 30 pictures of yourself, simply select them all and rename only one to the chosen identifier for example : phtmejhn, the files would be : phtmejhn (1).jpg, phtmejhn (2).png ....etc then upload them, do the same for other people or objects with a different identifier, and that's it.\n    #@markdown - Check out this example : https://i.imgur.com/d2lD3rz.jpeg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/action /kaggle/working/my","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@markdown #Instance Images\n#@markdown ----\n\n#@markdown\n#@markdown - Run the cell to Upload the instance pictures.\n\nRemove_existing_instance_images= True #@param{type: 'boolean'}\n#@markdown - Uncheck the box to keep the existing instance images.\n\n\nif Remove_existing_instance_images:\n  if os.path.exists(str(INSTANCE_DIR)):\n    !rm -r \"$INSTANCE_DIR\"\n\nif not os.path.exists(str(INSTANCE_DIR)):\n  %mkdir -p \"$INSTANCE_DIR\"\n\nIMAGES_FOLDER_OPTIONAL=\"/kaggle/working/my\" #@param{type: 'string'}\n\n#@markdown - If you prefer to specify directly the folder of the pictures instead of uploading, this will add the pictures to the existing (if any) instance images. Leave EMPTY to upload.\n\nCrop_images= True #@param{type: 'boolean'}\nCrop_size = \"512\" #@param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"]\nCrop_size=int(Crop_size)\n\n#@markdown - Unless you want to crop them manually in a precise way, you don't need to crop your instance images externally.\n\nwhile IMAGES_FOLDER_OPTIONAL !=\"\" and not os.path.exists(str(IMAGES_FOLDER_OPTIONAL)):\n  print('\u001b[1;31mThe image folder specified does not exist, use the colab file explorer to copy the path :')\n  IMAGES_FOLDER_OPTIONAL=input('')\n\nif IMAGES_FOLDER_OPTIONAL!=\"\":\n  if Crop_images:\n    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      extension = filename.split(\".\")[1]\n      identifier=filename.split(\".\")[0]\n      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n      file = Image.open(IMAGES_FOLDER_OPTIONAL+\"/\"+filename)\n      width, height = file.size\n      if file.size !=(Crop_size, Crop_size):      \n        side_length = min(width, height)\n        left = (width - side_length)/2\n        top = (height - side_length)/2\n        right = (width + side_length)/2\n        bottom = (height + side_length)/2\n        image = file.crop((left, top, right, bottom))\n        image = image.resize((Crop_size, Crop_size))\n        if (extension.upper() == \"JPG\"):\n            image.save(new_path_with_file, format=\"JPEG\", quality = 100)\n        else:\n            image.save(new_path_with_file, format=extension.upper())\n      else:\n        !cp \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n\n  else:\n    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      %cp -r \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n \n  print('\\n\u001b[1;32mDone, proceed to the training cell')\n\n\nelif IMAGES_FOLDER_OPTIONAL ==\"\":\n  uploaded = files.upload()\n  if Crop_images:\n    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      shutil.move(filename, INSTANCE_DIR)\n      extension = filename.split(\".\")[1]\n      identifier=filename.split(\".\")[0]\n      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n      file = Image.open(new_path_with_file)\n      width, height = file.size\n      if file.size !=(Crop_size, Crop_size):        \n        side_length = min(width, height)\n        left = (width - side_length)/2\n        top = (height - side_length)/2\n        right = (width + side_length)/2\n        bottom = (height + side_length)/2\n        image = file.crop((left, top, right, bottom))\n        image = image.resize((Crop_size, Crop_size))\n        if (extension.upper() == \"JPG\"):\n            image.save(new_path_with_file, format=\"JPEG\", quality = 100)\n        else:\n            image.save(new_path_with_file, format=extension.upper())\n      else:\n          image.save(new_path_with_file, format=extension.upper())\n      clear_output()\n  else:\n    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      shutil.move(filename, INSTANCE_DIR)\n      clear_output()\n\n  print('\\n\u001b[1;32mDone, proceed to the training cell')\n\nwith capture.capture_output() as cap:\n  %cd \"$INSTANCE_DIR\"\n  !find . -name \"* *\" -type f | rename 's/ /-/g'\n  %cd /kaggle/working/content\n  if os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n    %rm -r INSTANCE_DIR+\"/.ipynb_checkpoints\"    \n\n  %cd $SESSION_DIR\n  !rm instance_images.zip\n  !zip -r instance_images instance_images\n  %cd /kaggle/working/content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install diffusers\"[training]\" accelerate \"transformers>=4.4.2\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install diffusers\"[training]\" accelerate \"transformers>=4.4.2\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install ftfy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/content/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ftfy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@markdown ---\n#@markdown #Start DreamBooth\n#@markdown ---\nimport os\nfrom subprocess import getoutput\nfrom IPython.display import HTML\nfrom IPython.display import clear_output\nimport time\nimport random\n\nResume_Training = False #@param {type:\"boolean\"}\n\ntry:\n   resume\n   if resume and not Resume_Training:\n     print('\u001b[1;31mOverwrite your previously trained model ?, answering \"yes\" will train a new model, answering \"no\" will resume the training of the previous model?  yes or no ?\u001b[0m')\n     while True:\n        ansres=input('')\n        if ansres=='no':\n          Resume_Training = True\n          del ansres\n          break\n        elif ansres=='yes':\n          Resume_Training = False\n          resume= False\n          break\nexcept:\n  pass\n\nwhile not Resume_Training and MODEL_NAME==\"\":\n  print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n  time.sleep(5)\n\n#@markdown  - If you're not satisfied with the result, check this box, run again the cell and it will continue training the current model.\n\nMODELT_NAME=MODEL_NAME\n\nTraining_Steps=6000 #@param{type: 'number'}\n#@markdown - Total Steps = Number of Instance images * 200, if you use 30 images, use 6000 steps, if you're not satisfied with the result, resume training for another 500 steps, and so on ...\n\nSeed='' #@param{type: 'string'}\n\n#@markdown - Leave empty for a random seed.\n\nResolution = \"512\" #@param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"]\nRes=int(Resolution)\n\n#@markdown - Higher resolution = Higher quality, make sure the instance images are cropped to this selected size (or larger).\n\nfp16 = True #@param {type:\"boolean\"}\n\n#@markdown - Enable/disable half-precision, disabling it will double the training time and produce 4GB-5.2GB checkpoints.\n\n#GC= \"\"\n#if Resolution!=\"512\":\nGC= \"--gradient_checkpointing\"\n\nif Seed =='' or Seed=='0':\n  Seed=random.randint(1, 999999)\nelse:\n  Seed=int(Seed)\n\nif fp16:\n  prec=\"fp16\"\nelse:\n  prec=\"no\"\n\ns = getoutput('nvidia-smi')\nif 'A100' in s:\n  precision=\"no\"\n  GC= \"\"\nelse:\n  precision=prec\n\n\nif Resume_Training and os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n  MODELT_NAME=OUTPUT_DIR\n  print('\u001b[1;32mResuming Training...\u001b[0m')\nelif Resume_Training and not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n  print('\u001b[1;31mPrevious model not found, training a new model...\u001b[0m')\n  MODELT_NAME=MODEL_NAME\n  while MODEL_NAME==\"\":\n    print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n    time.sleep(5)\n\nif os.path.getsize(MODELT_NAME+\"/text_encoder/pytorch_model.bin\") > 670901463:\n  V2=True\n\n#@markdown ---------------------------\n\ntry:\n   Contain_f\n   pass\nexcept:\n   Contain_f=Contains_faces\n\nEnable_text_encoder_training= True #@param{type: 'boolean'}\n\n#@markdown - At least 10% of the total training steps are needed, it doesn't matter if they are at the beginning or in the middle or the end, in case you're training the model multiple times.\n#@markdown - For example you can devide 5%, 5%, 5% on 3 training runs on the model, or 0%, 0%, 15%, given that 15% will cover the total training steps count (15% of 200 steps is not enough).\n\n#@markdown - Enter the % of the total steps for which to train the text_encoder\nTrain_text_encoder_for=100 #@param{type: 'number'}\n\n#@markdown - If you're training a style, keep it between 10-20%, if you're training on a person, set it between 50-70%, reduce it if you can't stylize the person/object.\n#@markdown - Higher % will give more weight to the instance, it gives stronger results at lower steps count, but harder to stylize.\n\nif Train_text_encoder_for>=100:\n  stptxt=Training_Steps\nelif Train_text_encoder_for==0:\n  Enable_text_encoder_training= False\n  stptxt=10\nelse:\n  stptxt=int((Training_Steps*Train_text_encoder_for)/100)\n\nif not Enable_text_encoder_training:\n  Contains_faces=\"No\"\nelse:\n   Contains_faces=Contain_f\n\nif Enable_text_encoder_training:\n  Textenc=\"--train_text_encoder\"\nelse:\n  Textenc=\"\"\n\n#@markdown ---------------------------\nSave_Checkpoint_Every_n_Steps = False #@param {type:\"boolean\"}\nSave_Checkpoint_Every=500 #@param{type: 'number'}\nif Save_Checkpoint_Every==None:\n  Save_Checkpoint_Every=1\n#@markdown - Minimum 200 steps between each save.\nstp=0\nStart_saving_from_the_step=500 #@param{type: 'number'}\nif Start_saving_from_the_step==None:\n  Start_saving_from_the_step=0\nif (Start_saving_from_the_step < 200):\n  Start_saving_from_the_step=Save_Checkpoint_Every\nstpsv=Start_saving_from_the_step\nif Save_Checkpoint_Every_n_Steps:\n  stp=Save_Checkpoint_Every\n#@markdown - Start saving intermediary checkpoints from this step.\n\nDisconnect_after_training=False #@param {type:\"boolean\"}\n\n#@markdown - Auto-disconnect from google colab after the training to avoid wasting compute units.\n\ndef txtenc_train(MODELT_NAME, INSTANCE_DIR, CLASS_DIR, OUTPUT_DIR, PT, Seed, precision, GC, Training_Steps):\n    print('\u001b[1;33mTraining the text encoder with regularization...\u001b[0m')\n    !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    --image_captions_filename \\\n    --train_text_encoder \\\n    --dump_only_text_encoder \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --class_data_dir=\"$CLASS_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --with_prior_preservation --prior_loss_weight=1.0 \\\n    --instance_prompt=\"$PT\"\\\n    --seed=$Seed \\\n    --resolution=512 \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 $GC \\\n    --use_8bit_adam \\\n    --learning_rate=2e-6 \\\n    --lr_scheduler=\"polynomial\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps \\\n    --num_class_images=200\n\ndef unet_train(SESSION_DIR, stpsv, stp, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, GC, Training_Steps):\n    clear_output()\n    print('\u001b[1;33mTraining the UNet...\u001b[0m')\n    !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    --image_captions_filename \\\n    --train_only_unet \\\n    --Session_dir=$SESSION_DIR \\\n    --save_starting_step=$stpsv \\\n    --save_n_steps=$stp \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --instance_prompt=\"$PT\" \\\n    --seed=$Seed \\\n    --resolution=$Res \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 $GC \\\n    --use_8bit_adam \\\n    --learning_rate=2e-6 \\\n    --lr_scheduler=\"polynomial\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps\n\n\ndef train_only_textenc(MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps):\n    print('\u001b[1;32mV2 + Standard GPU detected.\u001b[0m')\n    print('\u001b[1;33mTraining the text encoder...\u001b[0m')\n    !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    --image_captions_filename \\\n    --train_text_encoder \\\n    --dump_only_text_encoder \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --instance_prompt=\"$PT\" \\\n    --seed=$Seed \\\n    --resolution=512 \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 --gradient_checkpointing \\\n    --use_8bit_adam \\\n    --learning_rate=2e-6 \\\n    --lr_scheduler=\"polynomial\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps\n\ndef train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps):\n    clear_output()\n    print('\u001b[1;33mTraining the UNet...\u001b[0m')\n    !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    --image_captions_filename \\\n    --train_only_unet \\\n    --save_starting_step=$stpsv \\\n    --save_n_steps=$stp \\\n    --Session_dir=$SESSION_DIR \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --instance_prompt=\"$PT\" \\\n    --seed=$Seed \\\n    --resolution=$Res \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 --gradient_checkpointing \\\n    --use_8bit_adam \\\n    --learning_rate=2e-6 \\\n    --lr_scheduler=\"polynomial\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps\n\nif Contains_faces!=\"No\":  \n  if Enable_text_encoder_training :\n    txtenc_train(MODELT_NAME, INSTANCE_DIR, CLASS_DIR, OUTPUT_DIR, PT, Seed, precision, GC, Training_Steps=stptxt)\n  unet_train(SESSION_DIR, stpsv, stp, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, GC, Training_Steps)\n\nelif V2 and Resolution!=\"512\" and not(\"A100\" in s):\n    if Enable_text_encoder_training :\n      train_only_textenc(MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps=stptxt)\n    train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps)\n    \n\nelse:\n  !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    $Textenc \\\n    --image_captions_filename \\\n    --save_starting_step=$stpsv \\\n    --stop_text_encoder_training=$stptxt \\\n    --save_n_steps=$stp \\\n    --Session_dir=$SESSION_DIR \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --instance_prompt=\"$PT\" \\\n    --seed=$Seed \\\n    --resolution=$Res \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 $GC \\\n    --use_8bit_adam \\\n    --learning_rate=2e-6 \\\n    --lr_scheduler=\"polynomial\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps\n\nimport shutil\nshutil.rmtree(\"/kaggle/working/content/stable-diffusion-v1-5\")\n\nif os.path.exists('/kaggle/working/content/models/'+INSTANCE_NAME+'/unet/diffusion_pytorch_model.bin'):\n  %cd /kaggle/working/content    \n  !wget -O convertosd.py https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/convertosd.py\n  clear_output()\n  if precision==\"no\":\n    !sed -i '226s@.*@@' /kaggle/working/content/convertosd.py\n  !sed -i '201s@.*@    model_path = \"{OUTPUT_DIR}\"@' /kaggle/working/content/convertosd.py\n  !sed -i '202s@.*@    checkpoint_path= \"{SESSION_DIR}/{Session_Name}.ckpt\"@' /kaggle/working/content/convertosd.py\n  !python /kaggle/working/content/convertosd.py\n  clear_output()\n  if V2:\n    print(\"\u001b[1;32mSaving the diffusers model to your gdrive...\")\n    !cp -r $OUTPUT_DIR $SESSION_DIR\n  if os.path.exists(SESSION_DIR+\"/\"+INSTANCE_NAME+'.ckpt'):\n    if not os.path.exists(str(SESSION_DIR+'/tokenizer')) and not V2:\n      !cp -R '/kaggle/working/content/models/'$INSTANCE_NAME'/tokenizer' \"$SESSION_DIR\"\n    print(\"\u001b[1;32mDONE, the CKPT model is in your Gdrive in the sessions folder\")\n    if Disconnect_after_training :\n      runtime.unassign()\n  else:\n    print(\"\u001b[1;31mSomething went wrong\")\n    \nelse:\n  print(\"\u001b[1;31mSomething went wrong\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cp /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/your_instance_name/your_instance_name.ckpt /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'actionf.ckpt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}