{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/robertodefano/fast-dreambooth-sd1-5-v2-1-models?scriptVersionId=131896505\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive/MyDrive","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working/content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow-io-gcs-filesystem==0.31.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install google-cloud-bigquery-storage","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wget","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@markdown # Dependencies\nfrom subprocess import getoutput\nimport time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/content/\n   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" !pip install -qq --no-deps accelerate==0.12.0\n   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" !wget -q -i https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dependencies/dbdeps.txt\n  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"  !dpkg -i *.deb\n   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" !tar -C / --zstd -xf gcolabdeps.tar.zst\n   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" !rm *.deb | rm *.zst | rm *.txt\n   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" !git clone -q --depth 1 --branch main https://github.com/TheLastBen/diffusers\n   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" !pip install gradio==3.16.2 --no-deps -qq  \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env LD_PRELOAD=libtcmalloc.so\n   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['PYTHONWARNINGS'] = 'ignore'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('•[1;32mDone, proceed')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get install git-lfs\n!git lfs install","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nfrom IPython.display import clear_output\nfrom IPython.utils import capture\nimport wget\nfrom subprocess import check_output\nimport urllib.request\nimport base64\n\n#@markdown - Skip this cell if you are loading a previous session\n\n#@markdown ---\n\nModel_Version = \"1.5\" #@param [ \"1.5\", \"V2-512px\", \"V2-768px\"]\n\n#@markdown - Choose which version to finetune.\n\n#@markdown ---\n\nwith capture.capture_output() as cap: \n  %cd /kaggle/working/content/\n\n#@markdown ---\n\nPath_to_HuggingFace= \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n\n#@markdown - Load and finetune a model from Hugging Face, use the format \"profile/model\" like : runwayml/stable-diffusion-v1-5\n#@markdown - If the custom model is private or requires a token, create token.txt containing the token in \"Fast-Dreambooth\" folder in your gdrive.\n\nMODEL_PATH = \"\" #@param {type:\"string\"}\n\nMODEL_LINK = \"\" #@param {type:\"string\"}\n\nsafetensors = True #@param {type:\"boolean\"}\n\nsftnsr=\"\"\nif not safetensors:\n  modelnm=\"model.ckpt\"\nelse:\n  modelnm=\"model.safetensors\"\n  sftnsr=\"--from_safetensors\"\n\nif os.path.exists('/kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/token.txt'):\n  with open(\"/kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/token.txt\") as f:\n     token = f.read()\n  authe=f'https://USER:{token}@'\nelse:\n  authe=\"https://\"\n\ndef downloadmodel():\n\n  if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5'):\n    !rm -r /kaggle/working/content/stable-diffusion-v1-5\n  clear_output()\n\n  %cd /kaggle/working/content/\n  clear_output()\n  !mkdir /kaggle/working/content/stable-diffusion-v1-5\n  %cd /kaggle/working/content/stable-diffusion-v1-5\n  !git init\n  !git lfs install --system --skip-repo\n  !git remote add -f origin  \"https://huggingface.co/runwayml/stable-diffusion-v1-5\"\n  !git config core.sparsecheckout true\n  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nmodel_index.json\\n!vae/diffusion_pytorch_model.bin\\n!*.safetensors\" > .git/info/sparse-checkout\n  !git pull origin main\n  if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n    !wget -q -O vae/diffusion_pytorch_model.bin https://huggingface.co/stabilityai/sd-vae-ft-mse/resolve/main/diffusion_pytorch_model.bin\n    !rm -r .git\n    !rm model_index.json\n    time.sleep(1)    \n    wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n    %cd /kaggle/working/content/\n    clear_output()\n    print('\u001b[1;32mDONE !')\n  else:\n    while not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n         print('\u001b[1;31mSomething went wrong')\n         time.sleep(5)\n\ndef newdownloadmodel():\n\n  %cd /kaggle/working/content/\n  clear_output()\n  !mkdir /kaggle/working/content/stable-diffusion-v2-768\n  %cd /kaggle/working/content/stable-diffusion-v2-768\n  !git init\n  !git lfs install --system --skip-repo\n  !git remote add -f origin  \"https://huggingface.co/stabilityai/stable-diffusion-2-1\"\n  !git config core.sparsecheckout true\n  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nfeature_extractor\\nmodel_index.json\\n!*.safetensors\" > .git/info/sparse-checkout\n  !git pull origin main\n  !rm -r /kaggle/working/content/stable-diffusion-v2-768/.git\n  %cd /kaggle/working/content/\n  clear_output()\n  print('\u001b[1;32mDONE !')\n\n\ndef newdownloadmodelb():\n\n  %cd /kaggle/working/content/\n  clear_output()\n  !mkdir /kaggle/working/content/stable-diffusion-v2-512\n  %cd /kaggle/working/content/stable-diffusion-v2-512\n  !git init\n  !git lfs install --system --skip-repo\n  !git remote add -f origin  \"https://huggingface.co/stabilityai/stable-diffusion-2-1-base\"\n  !git config core.sparsecheckout true\n  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nfeature_extractor\\nmodel_index.json\\n!*.safetensors\" > .git/info/sparse-checkout\n  !git pull origin main\n  !rm -r /kaggle/working/content/stable-diffusion-v2-512/.git\n  %cd /kaggle/working/content/\n  clear_output()\n  print('\u001b[1;32mDONE !')\n\n\nif Path_to_HuggingFace != \"\":\n  if authe==\"https://\":\n    textenc= f\"{authe}huggingface.co/{Path_to_HuggingFace}/resolve/main/text_encoder/pytorch_model.bin\"\n    txtenc_size=urllib.request.urlopen(textenc).info().get('Content-Length', None)\n  else:\n    textenc= f\"https://huggingface.co/{Path_to_HuggingFace}/resolve/main/text_encoder/pytorch_model.bin\"\n    creds = base64.b64encode(f\"USER:{token}\".encode('utf-8')).decode('utf-8')\n    req=urllib.request.Request(textenc)\n    req.add_header('Authorization', f'Basic {creds}')\n    txtenc_size=urllib.request.urlopen(req).info().get('Content-Length', None)\n  if int(txtenc_size)> 670000000 :\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom'):\n      !rm -r /kaggle/working/content/stable-diffusion-custom\n    clear_output()\n    %cd /kaggle/working/content/\n    clear_output()\n    print(\"\u001b[1;32mV2\")\n    !mkdir /kaggle/working/content/stable-diffusion-custom\n    %cd /kaggle/working/content/stable-diffusion-custom\n    !git init\n    !git lfs install --system --skip-repo\n    !git remote add -f origin  \"{authe}huggingface.co/{Path_to_HuggingFace}\"\n    !git config core.sparsecheckout true\n    !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nfeature_extractor\\nmodel_index.json\\n!*.safetensors\" > .git/info/sparse-checkout\n    !git pull origin main\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n      !rm -r /kaggle/working/content/stable-diffusion-custom/.git\n      %cd /kaggle/working/content/ \n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"\n      clear_output()\n      print('\u001b[1;32mDONE !')\n    else:\n      while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n            print('\u001b[1;31mCheck the link you provided')\n            time.sleep(5)\n  else:\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom'):\n      !rm -r /kaggle/working/content/stable-diffusion-custom\n    clear_output()\n    %cd /kaggle/working/content/\n    clear_output()\n    print(\"\u001b[1;32mV1\")\n    !mkdir /kaggle/working/content/stable-diffusion-custom\n    %cd /kaggle/working/content/stable-diffusion-custom\n    !git init\n    !git lfs install --system --skip-repo\n    !git remote add -f origin  \"{authe}huggingface.co/{Path_to_HuggingFace}\"\n    !git config core.sparsecheckout true\n    !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nmodel_index.json\\n!*.safetensors\" > .git/info/sparse-checkout\n    !git pull origin main\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n      !rm -r /kaggle/working/content/stable-diffusion-custom/.git\n      !rm model_index.json\n      time.sleep(1)\n      wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n      %cd /kaggle/working/content/ \n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"\n      clear_output()\n      print('\u001b[1;32mDONE !')\n    else:\n      while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n            print('\u001b[1;31mCheck the link you provided')\n            time.sleep(5)\n\nelif MODEL_PATH !=\"\":\n  %cd /kaggle/working/content\n  clear_output()\n  if os.path.exists(str(MODEL_PATH)):\n    wget.download('https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/det.py')\n    print('\u001b[1;33mDetecting model version...')\n    Custom_Model_Version=check_output('python det.py '+sftnsr+' --MODEL_PATH '+MODEL_PATH, shell=True).decode('utf-8').replace('\\n', '')\n    clear_output()\n    print('\u001b[1;32m'+Custom_Model_Version+' Detected')    \n    !rm det.py\n    if Custom_Model_Version=='1.5':      \n      !wget -q -O config.yaml https://github.com/CompVis/stable-diffusion/raw/main/configs/stable-diffusion/v1-inference.yaml\n      !python /kaggle/working/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path $MODEL_PATH --dump_path stable-diffusion-custom --original_config_file config.yaml $sftnsr\n      !rm /kaggle/working/content/config.yaml\n\n    elif Custom_Model_Version=='V2.1-512px':\n      !wget -q -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv2.py\n      !python /kaggle/working/content/convertodiff.py \"$MODEL_PATH\" /kaggle/working/content/stable-diffusion-custom --v2 --reference_model stabilityai/stable-diffusion-2-1-base $sftnsr\n      !rm /kaggle/working/content/convertodiff.py\n\n    elif Custom_Model_Version=='V2.1-768px':\n      !wget -q -O convertodiff.py https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/convertodiffv2-768.py\n      !python /kaggle/working/content/convertodiff.py \"$MODEL_PATH\" /kaggle/working/content/stable-diffusion-custom --v2 --reference_model stabilityai/stable-diffusion-2-1 $sftnsr\n      !rm /kaggle/working/content/convertodiff.py\n\n\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n      clear_output()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"\n      print('\u001b[1;32mDONE !')\n    else:\n      !rm -r /kaggle/working/content/stable-diffusion-custom\n      while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n        print('\u001b[1;31mConversion error')\n        time.sleep(5)\n  else:\n    while not os.path.exists(str(MODEL_PATH)):\n       print('\u001b[1;31mWrong path, use the colab file explorer to copy the path')\n       time.sleep(5)\n\nelif MODEL_LINK !=\"\":\n    %cd /kaggle/working/content\n    clear_output()\n    !gdown --fuzzy -O $modelnm \"$MODEL_LINK\"\n    clear_output() \n    if os.path.exists(modelnm):\n      if os.path.getsize(modelnm) > 1810671599:\n        wget.download('https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/det.py')\n        print('\u001b[1;33mDetecting model version...')\n        Custom_Model_Version=check_output('python det.py '+sftnsr+' --MODEL_PATH '+modelnm, shell=True).decode('utf-8').replace('\\n', '')\n        clear_output()\n        print('\u001b[1;32m'+Custom_Model_Version+' Detected') \n        !rm det.py\n        if Custom_Model_Version=='1.5':\n          !wget -q -O config.yaml https://github.com/CompVis/stable-diffusion/raw/main/configs/stable-diffusion/v1-inference.yaml\n          !python /kaggle/working/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path $modelnm --dump_path stable-diffusion-custom --original_config_file config.yaml $sftnsr\n          !rm config.yaml\n\n        elif Custom_Model_Version=='V2.1-512px':\n          !wget -q -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv2.py\n          !python /kaggle/working/content/convertodiff.py $modelnm /kaggle/working/content/stable-diffusion-custom --v2 --reference_model stabilityai/stable-diffusion-2-1-base $sftnsr\n          !rm convertodiff.py\n\n        elif Custom_Model_Version=='V2.1-768px':\n          !wget -q -O convertodiff.py https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/convertodiffv2-768.py\n          !python /kaggle/working/content/convertodiff.py $modelnm /kaggle/working/content/stable-diffusion-custom --v2 --reference_model stabilityai/stable-diffusion-2-1 $sftnsr\n          !rm convertodiff.py\n\n\n        if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n          clear_output()\n          MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"\n          print('\u001b[1;32mDONE !')\n        else:\n          !rm -r stable-diffusion-custom\n          !rm $modelnm\n          while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n            print('\u001b[1;31mConversion error')\n            time.sleep(5)\n      else:\n        while os.path.getsize(modelnm) < 1810671599:\n           print('\u001b[1;31mWrong link, check that the link is valid')\n           time.sleep(5)\n\nelse:\n  if Model_Version==\"1.5\":\n    if not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5'):\n      downloadmodel()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v1-5\"\n    else:\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v1-5\"\n      print(\"\u001b[1;32mThe v1.5 model already exists, using this model.\")\n  elif Model_Version==\"V2.1-512px\":\n    if not os.path.exists('/kaggle/working/content/stable-diffusion-v2-512'):\n      newdownloadmodelb()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-512\"\n    else:\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-512\"\n      print(\"\u001b[1;32mThe v2-512px model already exists, using this model.\")\n  elif Model_Version==\"V2.1-768px\":\n    if not os.path.exists('/kaggle/working/content/stable-diffusion-v2-768'):\n      newdownloadmodel()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-768\"\n    else:\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-768\"\n      print(\"\u001b[1;32mThe v2-768px model already exists, using this model.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/instance_images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/Regularization_images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom IPython.display import clear_output\nfrom IPython.utils import capture\nfrom os import listdir\nfrom os.path import isfile\nfrom subprocess import check_output\nimport wget\nimport time\n\n#@markdown #Create/Load a Session\n\ntry:\n  MODEL_NAME\n  pass\nexcept:\n  MODEL_NAME=\"\"\n  \nPT=\"\"\n\nSession_Name = \"your_instance_name\" #@param{type: 'string'}\nwhile Session_Name==\"\":\n  print('\u001b[1;31mInput the Session Name:') \n  Session_Name=input('')\nSession_Name=Session_Name.replace(\" \",\"_\")\n\n#@markdown - Enter the session name, it if it exists, it will load it, otherwise it'll create an new session.\n\nSession_Link_optional = \"\" #@param{type: 'string'}\n\n#@markdown - Import a session from another gdrive, the shared gdrive link must point to the specific session's folder that contains the trained CKPT, remove any intermediary CKPT if any.\n\nWORKSPACE='/kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth'\n\nif Session_Link_optional !=\"\":\n  print('\u001b[1;32mDownloading session...')\n  with capture.capture_output() as cap:\n    %cd /kaggle/working/content\n    if not os.path.exists(str(WORKSPACE+'/Sessions')):\n      %mkdir -p $WORKSPACE'/Sessions'\n      time.sleep(1)\n    %cd $WORKSPACE'/Sessions'\n    !gdown --folder --remaining-ok -O $Session_Name  $Session_Link_optional\n    %cd $Session_Name\n    !rm -r instance_images\n    !unzip instance_images.zip\n    !rm -r concept_images\n    !unzip concept_images.zip\n    !rm -r captions\n    !unzip captions.zip\n    %cd /kaggle/working/content\n\n\nINSTANCE_NAME=Session_Name\nOUTPUT_DIR=\"/kaggle/working/content/models/\"+Session_Name\nSESSION_DIR=WORKSPACE+'/Sessions/'+Session_Name\nINSTANCE_DIR=SESSION_DIR+'/instance_images'\nCONCEPT_DIR=SESSION_DIR+'/concept_images'\nCAPTIONS_DIR=SESSION_DIR+'/captions'\nMDLPTH=str(SESSION_DIR+\"/\"+Session_Name+'.ckpt')\n\nif os.path.exists(str(SESSION_DIR)):\n  mdls=[ckpt for ckpt in listdir(SESSION_DIR) if ckpt.split(\".\")[-1]==\"ckpt\"]\n  if not os.path.exists(MDLPTH) and '.ckpt' in str(mdls):  \n    \n    def f(n):\n      k=0\n      for i in mdls:\n        if k==n:\n          !mv \"$SESSION_DIR/$i\" $MDLPTH\n        k=k+1\n\n    k=0\n    print('\u001b[1;33mNo final checkpoint model found, select which intermediary checkpoint to use, enter only the number, (000 to skip):\\n\u001b[1;34m')\n\n    for i in mdls:\n      print(str(k)+'- '+i)\n      k=k+1\n    n=input()\n    while int(n)>k-1:\n      n=input()\n    if n!=\"000\":\n      f(int(n))\n      print('\u001b[1;32mUsing the model '+ mdls[int(n)]+\" ...\")\n      time.sleep(2)\n    else:\n      print('\u001b[1;32mSkipping the intermediary checkpoints.')\n    del n\n\nwith capture.capture_output() as cap:\n  %cd /kaggle/working/content\n  resume=False\n\nif os.path.exists(str(SESSION_DIR)) and not os.path.exists(MDLPTH):\n  print('\u001b[1;32mLoading session with no previous model, using the original model or the custom downloaded model')\n  if MODEL_NAME==\"\":\n    print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n  else:\n    print('\u001b[1;32mSession Loaded, proceed to uploading instance images')\n\nelif os.path.exists(MDLPTH):\n  print('\u001b[1;32mSession found, loading the trained model ...')\n  wget.download('https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/det.py')\n  print('\u001b[1;33mDetecting model version...')\n  Model_Version=check_output('python det.py --MODEL_PATH '+MDLPTH, shell=True).decode('utf-8').replace('\\n', '')\n  clear_output()\n  print('\u001b[1;32m'+Model_Version+' Detected') \n  !rm det.py  \n  if Model_Version=='1.5':\n    !wget -q -O config.yaml https://github.com/CompVis/stable-diffusion/raw/main/configs/stable-diffusion/v1-inference.yaml\n    print('\u001b[1;32mSession found, loading the trained model ...')\n    !python /kaggle/working/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path $MDLPTH --dump_path \"$OUTPUT_DIR\" --original_config_file config.yaml\n    !rm /kaggle/working/content/config.yaml\n\n  elif Model_Version=='V2.1-512px':\n    !wget -q -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv2.py\n    print('\u001b[1;32mSession found, loading the trained model ...')\n    !python /kaggle/working/content/convertodiff.py \"$MDLPTH\" \"$OUTPUT_DIR\" --v2 --reference_model stabilityai/stable-diffusion-2-1-base\n    !rm /kaggle/working/content/convertodiff.py\n\n  elif Model_Version=='V2.1-768px':\n    !wget -q -O convertodiff.py https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/convertodiffv2-768.py\n    print('\u001b[1;32mSession found, loading the trained model ...')\n    !python /kaggle/working/content/convertodiff.py \"$MDLPTH\" \"$OUTPUT_DIR\" --v2 --reference_model stabilityai/stable-diffusion-2-1\n    !rm /kaggle/working/content/convertodiff.py\n  \n  \n  if os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n    resume=True\n    clear_output()\n    print('\u001b[1;32mSession loaded.')\n  else:     \n    if not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n      print('\u001b[1;31mConversion error, if the error persists, remove the CKPT file from the current session folder')\n\nelif not os.path.exists(str(SESSION_DIR)):\n    %mkdir -p \"$INSTANCE_DIR\"\n    print('\u001b[1;32mCreating session...')\n    if MODEL_NAME==\"\":\n      print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n    else:\n      print('\u001b[1;32mSession created, proceed to uploading instance images')\n\n    #@markdown\n\n    #@markdown # The most important step is to rename the instance pictures of each subject to a unique unknown identifier, example :\n    #@markdown - If you have 10 pictures of yourself, simply select them all and rename only one to the chosen identifier for example : phtmejhn, the files would be : phtmejhn (1).jpg, phtmejhn (2).png ....etc then upload them, do the same for other people or objects with a different identifier, and that's it.\n    #@markdown - Checkout this example : https://i.imgur.com/d2lD3rz.jpeg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ipywidgets as widgets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from io import BytesIO","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wget","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/your_instance_folder /kaggle/working/my","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with capture.capture_output() as cap:\n  %cd /kaggle/working/content\n  if not os.path.exists(\"/kaggle/working/content/smart_crop.py\"):\n    wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/smart_crop.py')\n  from smart_crop import *\n\n#@markdown #Instance Images\n#@markdown ----\n\n#@markdown\n#@markdown - Run the cell to upload the instance pictures.\n#@markdown - You can add `external captions` in txt files by simply giving each txt file the same name as the instance image, for example dikgur (1).jpg and dikgur (1).txt, and upload them here, to use the external captions, check the box \"external_captions\" in the training cell. `All the images must have one same extension` jpg or png or....etc\n\nRemove_existing_instance_images= True #@param{type: 'boolean'}\n#@markdown - Uncheck the box to keep the existing instance images.\n\nif Remove_existing_instance_images:\n  if os.path.exists(str(INSTANCE_DIR)):\n    !rm -r \"$INSTANCE_DIR\"\n  if os.path.exists(str(CAPTIONS_DIR)):\n    !rm -r \"$CAPTIONS_DIR\"\n\nif not os.path.exists(str(INSTANCE_DIR)):\n  %mkdir -p \"$INSTANCE_DIR\"\nif not os.path.exists(str(CAPTIONS_DIR)):\n  %mkdir -p \"$CAPTIONS_DIR\"\n\nif os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n  %rm -r $INSTANCE_DIR\"/.ipynb_checkpoints\"\n\n\nIMAGES_FOLDER_OPTIONAL=\"/kaggle/working/my\" #@param{type: 'string'}\n\n#@markdown - If you prefer to specify directly the folder of the pictures instead of uploading, this will add the pictures to the existing (if any) instance images. Leave EMPTY to upload.\n\nSmart_Crop_images= False #@param{type: 'boolean'}\nCrop_size = 512 #@param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"] {type:\"raw\"}\n\n#@markdown - Smart crop the images without manual intervention.\n\nwhile IMAGES_FOLDER_OPTIONAL !=\"\" and not os.path.exists(str(IMAGES_FOLDER_OPTIONAL)):\n  print('\u001b[1;31mThe image folder specified does not exist, use the colab file explorer to copy the path :')\n  IMAGES_FOLDER_OPTIONAL=input('')\n\nif IMAGES_FOLDER_OPTIONAL!=\"\":\n  if os.path.exists(IMAGES_FOLDER_OPTIONAL+\"/.ipynb_checkpoints\"):\n    %rm -r \"$IMAGES_FOLDER_OPTIONAL\"\"/.ipynb_checkpoints\"\n\n  with capture.capture_output() as cap:\n    !mv $IMAGES_FOLDER_OPTIONAL/*.txt $CAPTIONS_DIR\n  if Smart_Crop_images:\n    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      extension = filename.split(\".\")[-1]\n      identifier=filename.split(\".\")[0]\n      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n      file = Image.open(IMAGES_FOLDER_OPTIONAL+\"/\"+filename)\n      width, height = file.size\n      if file.size !=(Crop_size, Crop_size):\n        image=crop_image(file, Crop_size)\n        if extension.upper()==\"JPG\" or extension.upper()==\"jpg\":\n            image[0] = image[0].convert(\"RGB\")\n            image[0].save(new_path_with_file, format=\"JPEG\", quality = 100)\n        else:\n            image[0].save(new_path_with_file, format=extension.upper())\n      else:\n        !cp \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n\n  else:\n    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      %cp -r \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n\n  print('\\n\u001b[1;32mDone, proceed to the next cell')\n\n\nelif IMAGES_FOLDER_OPTIONAL ==\"\":\n  up=\"\"\n  uploaded = files.upload()\n  for filename in uploaded.keys():\n    if filename.split(\".\")[-1]==\"txt\":\n      shutil.move(filename, CAPTIONS_DIR)\n    up=[filename for filename in uploaded.keys() if filename.split(\".\")[-1]!=\"txt\"]\n  if Smart_Crop_images:\n    for filename in tqdm(up, bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      shutil.move(filename, INSTANCE_DIR)\n      extension = filename.split(\".\")[-1]\n      identifier=filename.split(\".\")[0]\n      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n      file = Image.open(new_path_with_file)\n      width, height = file.size\n      if file.size !=(Crop_size, Crop_size):\n        image=crop_image(file, Crop_size)\n        if extension.upper()==\"JPG\" or extension.upper()==\"jpg\":\n            image[0] = image[0].convert(\"RGB\")\n            image[0].save(new_path_with_file, format=\"JPEG\", quality = 100)\n        else:\n            image[0].save(new_path_with_file, format=extension.upper())\n      clear_output()\n  else:\n    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      shutil.move(filename, INSTANCE_DIR)\n      clear_output()\n  print('\\n\u001b[1;32mDone, proceed to the next cell')\n\nwith capture.capture_output() as cap:\n  %cd \"$INSTANCE_DIR\"\n  !find . -name \"* *\" -type f | rename 's/ /-/g'\n  %cd \"$CAPTIONS_DIR\"\n  !find . -name \"* *\" -type f | rename 's/ /-/g'\n  \n  %cd $SESSION_DIR\n  !rm instance_images.zip captions.zip\n  !zip -r instance_images instance_images\n  !zip -r captions captions\n  %cd /kaggle/working/content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ipywidgets as widgets\nfrom io import BytesIO\n#@markdown #Captions (optional)\n\n#@markdown - Open a tool to manually `create` captions or edit existing captions of the instance images, do not use captions when training on a face.\n\npaths=\"\"\nout=\"\"\nwidgets_l=\"\"\nclear_output()\ndef Caption(path):\n    if path!=\"Select an instance image to caption\":\n      \n      name = os.path.splitext(os.path.basename(path))[0]\n      ext=os.path.splitext(os.path.basename(path))[-1][1:]\n      if ext==\"jpg\" or \"JPG\":\n        ext=\"JPEG\"      \n\n      if os.path.exists(CAPTIONS_DIR+\"/\"+name + '.txt'):\n        with open(CAPTIONS_DIR+\"/\"+name + '.txt', 'r') as f:\n            text = f.read()\n      else:\n        with open(CAPTIONS_DIR+\"/\"+name + '.txt', 'w') as f:\n            f.write(\"\")\n            with open(CAPTIONS_DIR+\"/\"+name + '.txt', 'r') as f:\n                text = f.read()   \n\n      img=Image.open(os.path.join(INSTANCE_DIR,path))\n      img=img.convert(\"RGB\")\n      img=img.resize((420, 420))\n      image_bytes = BytesIO()\n      img.save(image_bytes, format=ext, qualiy=10)\n      image_bytes.seek(0)\n      image_data = image_bytes.read()\n      img= image_data  \n      image = widgets.Image(\n          value=img,\n          width=420,\n          height=420\n      )\n      text_area = widgets.Textarea(value=text, description='', disabled=False, layout={'width': '300px', 'height': '120px'})\n      \n\n      def update_text(text):\n          with open(CAPTIONS_DIR+\"/\"+name + '.txt', 'w') as f:\n              f.write(text)\n\n      button = widgets.Button(description='Save', button_style='success')\n      button.on_click(lambda b: update_text(text_area.value))\n\n      return widgets.VBox([widgets.HBox([image, text_area, button])])\n\n\npaths = os.listdir(INSTANCE_DIR)\nwidgets_l = widgets.Select(options=[\"Select an instance image to caption\"]+paths, rows=25)\n\n\nout = widgets.Output()\n\ndef click(change):\n    with out:\n        out.clear_output()\n        display(Caption(change.new))\n\nwidgets_l.observe(click, names='value')\ndisplay(widgets.HBox([widgets_l, out]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@markdown #Concept Images (Regularization)\n#@markdown ----\n\n#@markdown\n#@markdown - Run this `optional` cell to upload concept pictures. If you're traning on a specific face, skip this cell.\n\nwith capture.capture_output() as cap:\n  %cd /kaggle/working/content\n  if not os.path.exists(\"/kaggle/working/content/smart_crop.py\"):\n    wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/smart_crop.py')\n  from smart_crop import *\n\nRemove_existing_concept_images= True #@param{type: 'boolean'}\n#@markdown - Uncheck the box to keep the existing concept images.\n\nif Remove_existing_concept_images:\n  if os.path.exists(str(CONCEPT_DIR)):\n    !rm -r \"$CONCEPT_DIR\"\n\nif not os.path.exists(str(CONCEPT_DIR)):\n  %mkdir -p \"$CONCEPT_DIR\"\n\nIMAGES_FOLDER_OPTIONAL=\"/kaggle/working/my\" #@param{type: 'string'}\n\n#@markdown - If you prefer to specify directly the folder of the pictures instead of uploading, this will add the pictures to the existing (if any) concept images. Leave EMPTY to upload.\n\nSmart_Crop_images= True\nCrop_size = 512\n\nwhile IMAGES_FOLDER_OPTIONAL !=\"\" and not os.path.exists(str(IMAGES_FOLDER_OPTIONAL)):\n  print('\u001b[1;31mThe image folder specified does not exist, use the colab file explorer to copy the path :')\n  IMAGES_FOLDER_OPTIONAL=input('')\n\nif IMAGES_FOLDER_OPTIONAL!=\"\":\n  if Smart_Crop_images:\n    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      extension = filename.split(\".\")[-1]\n      identifier=filename.split(\".\")[0]\n      new_path_with_file = os.path.join(CONCEPT_DIR, filename)\n      file = Image.open(IMAGES_FOLDER_OPTIONAL+\"/\"+filename)\n      width, height = file.size\n      if file.size !=(Crop_size, Crop_size):\n        image=crop_image(file, Crop_size)\n        if extension.upper() == \"JPG\" or \"jpg\":\n            image[0].save(new_path_with_file, format=\"JPEG\", quality = 100)\n        else:\n            image[0].save(new_path_with_file, format=extension.upper())\n      else:\n        !cp \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$CONCEPT_DIR\"\n\n  else:\n    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      %cp -r \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$CONCEPT_DIR\"\n\nelif IMAGES_FOLDER_OPTIONAL ==\"\":\n  uploaded = files.upload()\n  if Smart_Crop_images:\n    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      shutil.move(filename, CONCEPT_DIR)\n      extension = filename.split(\".\")[-1]\n      identifier=filename.split(\".\")[0]\n      new_path_with_file = os.path.join(CONCEPT_DIR, filename)\n      file = Image.open(new_path_with_file)\n      width, height = file.size\n      if file.size !=(Crop_size, Crop_size):\n        image=crop_image(file, Crop_size)\n        if extension.upper() == \"JPG\" or \"jpg\":\n            image[0].save(new_path_with_file, format=\"JPEG\", quality = 100)\n        else:\n            image[0].save(new_path_with_file, format=extension.upper())\n      clear_output()\n  else:\n    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      shutil.move(filename, CONCEPT_DIR)\n      clear_output()\n\n\nprint('\\n\u001b[1;32mAlmost done...')\nwith capture.capture_output() as cap:\n  i=0\n  for filename in os.listdir(CONCEPT_DIR):\n    extension = filename.split(\".\")[-1]\n    identifier=filename.split(\".\")[0]\n    new_path_with_file = os.path.join(CONCEPT_DIR, \"conceptimagedb\"+str(i)+\".\"+extension)\n    filepath=os.path.join(CONCEPT_DIR,filename)\n    !mv \"$filepath\" $new_path_with_file\n    i=i+1\n\n  %cd $SESSION_DIR\n  !rm concept_images.zip\n  !zip -r concept_images concept_images\n  %cd /kaggle/working/content\n\nprint('\\n\u001b[1;32mDone, proceed to the training cell')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install diffusers\"[training]\" accelerate \"transformers>=4.4.2\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install ftfy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/content/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ftfy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pkg_resources\npkg_resources.get_distribution('transformers').version","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install transformers -U","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install xformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torch==2.0.1+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@markdown ---\n#@markdown #Start DreamBooth\n#@markdown ---\nimport os\nfrom subprocess import getoutput\nfrom IPython.display import HTML\nfrom IPython.display import clear_output\nimport time\nimport random\n\nif os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n  %rm -r $INSTANCE_DIR\"/.ipynb_checkpoints\"\n\nif os.path.exists(CONCEPT_DIR+\"/.ipynb_checkpoints\"):\n  %rm -r $CONCEPT_DIR\"/.ipynb_checkpoints\"\n\nif os.path.exists(CAPTIONS_DIR+\"/.ipynb_checkpoints\"):\n  %rm -r $CAPTIONS_DIR\"/.ipynb_checkpoints\"\n\nResume_Training = False #@param {type:\"boolean\"}\n\nif resume and not Resume_Training:\n  print('\u001b[1;31mOverwrite your previously trained model ? answering \"yes\" will train a new model, answering \"no\" will resume the training of the previous model?  yes or no ?\u001b[0m')\n  while True:\n    ansres=input('')\n    if ansres=='no':\n      Resume_Training = True\n      break\n    elif ansres=='yes':\n      Resume_Training = False\n      resume= False\n      break\n\nwhile not Resume_Training and MODEL_NAME==\"\":\n  print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n  time.sleep(5)\n\n#@markdown  - If you're not satisfied with the result, check this box, run again the cell and it will continue training the current model.\n\nMODELT_NAME=MODEL_NAME\n\nUNet_Training_Steps=9000 #@param{type: 'number'}\nUNet_Learning_Rate = 2e-6 #@param [\"2e-5\",\"1e-5\",\"9e-6\",\"8e-6\",\"7e-6\",\"6e-6\",\"5e-6\", \"4e-6\", \"3e-6\", \"2e-6\"] {type:\"raw\"}\nuntlr=UNet_Learning_Rate\n\n#@markdown - These default settings are for a dataset of 10 pictures which is enough for training a face, start with 1500 or lower, test the model, if not enough, resume training for 200 steps, keep testing until you get the desired output, `set it to 0 to train only the text_encoder`.\n\nText_Encoder_Training_Steps=150 #@param{type: 'number'}\n\n#@markdown - 200-450 steps is enough for a small dataset, keep this number small to avoid overfitting, set to 0 to disable, `set it to 0 before resuming training if it is already trained`.\n\nText_Encoder_Learning_Rate = 2e-6 #@param [\"2e-6\", \"1e-6\",\"8e-7\",\"6e-7\",\"5e-7\",\"4e-7\"] {type:\"raw\"}\ntxlr=Text_Encoder_Learning_Rate\n\n#@markdown - Learning rate for both text_encoder and concept_text_encoder, keep it low to avoid overfitting (1e-6 is higher than 4e-7)\n\nText_Encoder_Concept_Training_Steps=0 #@param{type: 'number'}\n\n#@markdown - Suitable for training a style/concept as it acts as heavy regularization, set it to 1500 steps for 200 concept images (you can go higher), set to 0 to disable, set both the settings above to 0 to fintune only the text_encoder on the concept, `set it to 0 before resuming training if it is already trained`.\n\ntrnonltxt=\"\"\nif UNet_Training_Steps==0:\n   trnonltxt=\"--train_only_text_encoder\"\n\nSeed=''\n\nofstnse=\"\"\nOffset_Noise = True #@param {type:\"boolean\"}\n#@markdown - Always use it for style training.\n\nif Offset_Noise:\n  ofstnse=\"--offset_noise\"\n\nExternal_Captions = False #@param {type:\"boolean\"}\n#@markdown - Get the captions from a text file for each instance image.\nextrnlcptn=\"\"\nif External_Captions:\n  extrnlcptn=\"--external_captions\"\n\nResolution = \"512\" #@param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"]\nRes=int(Resolution)\n\n#@markdown - Higher resolution = Higher quality, make sure the instance images are cropped to this selected size (or larger).\n\nfp16 = True\n\nif Seed =='' or Seed=='0':\n  Seed=random.randint(1, 999999)\nelse:\n  Seed=int(Seed)\n\nif fp16:\n  prec=\"fp16\"\nelse:\n  prec=\"no\"\n\nprecision=prec\n\nresuming=\"\"\nif Resume_Training and os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n  MODELT_NAME=OUTPUT_DIR\n  print('\u001b[1;32mResuming Training...\u001b[0m')\n  resuming=\"Yes\"\nelif Resume_Training and not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n  print('\u001b[1;31mPrevious model not found, training a new model...\u001b[0m')\n  MODELT_NAME=MODEL_NAME\n  while MODEL_NAME==\"\":\n    print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n    time.sleep(5)\n\nV2=False\nif os.path.getsize(MODELT_NAME+\"/text_encoder/pytorch_model.bin\") > 670901463:\n  V2=True\n\ns = getoutput('nvidia-smi')\nGCUNET=\"--gradient_checkpointing\"\nTexRes=Res\nif Res<=768:\n  GCUNET=\"\"\n\nif V2:  \n  if Res>704:\n    GCUNET=\"--gradient_checkpointing\"\n  if Res>576:\n    TexRes=576\n\nif 'A100' in s :\n   GCUNET=\"\"\n   TexRes=Res\n\n\nEnable_text_encoder_training= True\nEnable_Text_Encoder_Concept_Training= True\n\nif Text_Encoder_Training_Steps==0 :\n   Enable_text_encoder_training= False\nelse:\n  stptxt=Text_Encoder_Training_Steps\n\nif Text_Encoder_Concept_Training_Steps==0:\n   Enable_Text_Encoder_Concept_Training= False\nelse:\n  stptxtc=Text_Encoder_Concept_Training_Steps\n\n#@markdown ---------------------------\nSave_Checkpoint_Every_n_Steps = False #@param {type:\"boolean\"}\nSave_Checkpoint_Every=500 #@param{type: 'number'}\nif Save_Checkpoint_Every==None:\n  Save_Checkpoint_Every=1\n#@markdown - Minimum 200 steps between each save.\nstp=0\nStart_saving_from_the_step=500 #@param{type: 'number'}\nif Start_saving_from_the_step==None:\n  Start_saving_from_the_step=0\nif (Start_saving_from_the_step < 200):\n  Start_saving_from_the_step=Save_Checkpoint_Every\nstpsv=Start_saving_from_the_step\nif Save_Checkpoint_Every_n_Steps:\n  stp=Save_Checkpoint_Every\n#@markdown - Start saving intermediary checkpoints from this step.\n\nDisconnect_after_training=False #@param {type:\"boolean\"}\n\n#@markdown - Auto-disconnect from google colab after the training to avoid wasting compute units.\n\ndef dump_only_textenc(trnonltxt, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps):\n    \n    !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    $trnonltxt \\\n    $extrnlcptn \\\n    $ofstnse \\\n    --image_captions_filename \\\n    --train_text_encoder \\\n    --dump_only_text_encoder \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --captions_dir=\"$CAPTIONS_DIR\" \\\n    --instance_prompt=\"$PT\" \\\n    --seed=$Seed \\\n    --resolution=$TexRes \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 --gradient_checkpointing \\\n    --use_8bit_adam \\\n    --learning_rate=$txlr \\\n    --lr_scheduler=\"linear\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps\n\ndef train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps):\n    clear_output()\n    if resuming==\"Yes\":\n      print('\u001b[1;32mResuming Training...\u001b[0m')\n    print('\u001b[1;33mTraining the UNet...\u001b[0m')\n    !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    $extrnlcptn \\\n    $ofstnse \\\n    --image_captions_filename \\\n    --train_only_unet \\\n    --save_starting_step=$stpsv \\\n    --save_n_steps=$stp \\\n    --Session_dir=$SESSION_DIR \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --captions_dir=\"$CAPTIONS_DIR\" \\\n    --instance_prompt=\"$PT\" \\\n    --seed=$Seed \\\n    --resolution=$Res \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 $GCUNET \\\n    --use_8bit_adam \\\n    --learning_rate=$untlr \\\n    --lr_scheduler=\"linear\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps\n\n\nif Enable_text_encoder_training :\n  print('\u001b[1;33mTraining the text encoder...\u001b[0m')\n  if os.path.exists(OUTPUT_DIR+'/'+'text_encoder_trained'):\n    %rm -r $OUTPUT_DIR\"/text_encoder_trained\"\n  dump_only_textenc(trnonltxt, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps=stptxt)\n\nif Enable_Text_Encoder_Concept_Training:\n  if os.path.exists(CONCEPT_DIR):\n    if os.listdir(CONCEPT_DIR)!=[]:\n      clear_output()\n      if resuming==\"Yes\":\n        print('\u001b[1;32mResuming Training...\u001b[0m')\n      print('\u001b[1;33mTraining the text encoder on the concept...\u001b[0m')\n      dump_only_textenc(trnonltxt, MODELT_NAME, CONCEPT_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps=stptxtc)\n    else:\n      clear_output()\n      if resuming==\"Yes\":\n        print('\u001b[1;32mResuming Training...\u001b[0m')\n      print('\u001b[1;31mNo concept images found, skipping concept training...')\n      Text_Encoder_Concept_Training_Steps=0\n      time.sleep(8)\n  else:\n      clear_output()\n      if resuming==\"Yes\":\n        print('\u001b[1;32mResuming Training...\u001b[0m')\n      print('\u001b[1;31mNo concept images found, skipping concept training...')\n      Text_Encoder_Concept_Training_Steps=0\n      time.sleep(8)\n\nif UNet_Training_Steps!=0:\n  train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps=UNet_Training_Steps)\n\nif UNet_Training_Steps==0 and Text_Encoder_Concept_Training_Steps==0 and Text_Encoder_Training_Steps==0 :\n  print('\u001b[1;32mNothing to do')\nelse:\n  if os.path.exists('/kaggle/working/content/models/'+INSTANCE_NAME+'/unet/diffusion_pytorch_model.bin'):\n    prc=\"--fp16\" if precision==\"fp16\" else \"\"\n    !python /kaggle/working/content/diffusers/scripts/convertosdv2.py $prc $OUTPUT_DIR $SESSION_DIR/$Session_Name\".ckpt\"\n    clear_output()\n    if os.path.exists(SESSION_DIR+\"/\"+INSTANCE_NAME+'.ckpt'):\n      clear_output()\n      print(\"\u001b[1;32mDONE, the CKPT model is in your Gdrive in the sessions folder\")\n      if Disconnect_after_training :\n        time.sleep(20)\n        runtime.unassign()\n    else:\n      print(\"\u001b[1;31mSomething went wrong\")\n  else:\n    print(\"\u001b[1;31mSomething went wrong\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cp /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/your_instance_name/your_instance_name.ckpt /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'your_instance_name.ckpt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}